{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.249999</td>\n",
       "      <td>-1.221637</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>-1.234899</td>\n",
       "      <td>-1.485419</td>\n",
       "      <td>-0.753230</td>\n",
       "      <td>-0.689405</td>\n",
       "      <td>-0.227487</td>\n",
       "      <td>-2.094011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231809</td>\n",
       "      <td>-0.483285</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.161135</td>\n",
       "      <td>-0.354990</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>121.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.069374</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>2.712520</td>\n",
       "      <td>-0.178398</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.096717</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036876</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.071407</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.548265</td>\n",
       "      <td>0.104094</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.021293</td>\n",
       "      <td>27.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.791855</td>\n",
       "      <td>-0.327771</td>\n",
       "      <td>1.641750</td>\n",
       "      <td>1.767473</td>\n",
       "      <td>-0.136588</td>\n",
       "      <td>0.807596</td>\n",
       "      <td>-0.422911</td>\n",
       "      <td>-1.907107</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151663</td>\n",
       "      <td>0.222182</td>\n",
       "      <td>1.020586</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>-0.235557</td>\n",
       "      <td>-0.164778</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>58.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.752417</td>\n",
       "      <td>0.345485</td>\n",
       "      <td>2.057323</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.158394</td>\n",
       "      <td>-0.077850</td>\n",
       "      <td>-0.608581</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>-0.436167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>-0.087086</td>\n",
       "      <td>-0.180998</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>15.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.436905</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>-0.727219</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>-0.665271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194796</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-0.156858</td>\n",
       "      <td>-0.888386</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>-0.049027</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.492936</td>\n",
       "      <td>-1.029346</td>\n",
       "      <td>0.454795</td>\n",
       "      <td>-1.438026</td>\n",
       "      <td>-1.555434</td>\n",
       "      <td>-0.720961</td>\n",
       "      <td>-1.080664</td>\n",
       "      <td>-0.053127</td>\n",
       "      <td>-1.978682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177650</td>\n",
       "      <td>-0.175074</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.332931</td>\n",
       "      <td>-0.220385</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.694885</td>\n",
       "      <td>-1.361819</td>\n",
       "      <td>1.029221</td>\n",
       "      <td>0.834159</td>\n",
       "      <td>-1.191209</td>\n",
       "      <td>1.309109</td>\n",
       "      <td>-0.878586</td>\n",
       "      <td>0.445290</td>\n",
       "      <td>-0.446196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295583</td>\n",
       "      <td>-0.571955</td>\n",
       "      <td>-0.050881</td>\n",
       "      <td>-0.304215</td>\n",
       "      <td>0.072001</td>\n",
       "      <td>-0.422234</td>\n",
       "      <td>0.086553</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>231.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.166616</td>\n",
       "      <td>0.502120</td>\n",
       "      <td>-0.067300</td>\n",
       "      <td>2.261569</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.241147</td>\n",
       "      <td>0.138082</td>\n",
       "      <td>-0.989162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.061972</td>\n",
       "      <td>-0.103855</td>\n",
       "      <td>-0.370415</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.108556</td>\n",
       "      <td>-0.040521</td>\n",
       "      <td>-0.011418</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.247491</td>\n",
       "      <td>0.277666</td>\n",
       "      <td>1.185471</td>\n",
       "      <td>-0.092603</td>\n",
       "      <td>-1.314394</td>\n",
       "      <td>-0.150116</td>\n",
       "      <td>-0.946365</td>\n",
       "      <td>-1.617935</td>\n",
       "      <td>1.544071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.650180</td>\n",
       "      <td>0.200454</td>\n",
       "      <td>-0.185353</td>\n",
       "      <td>0.423073</td>\n",
       "      <td>0.820591</td>\n",
       "      <td>-0.227632</td>\n",
       "      <td>0.336634</td>\n",
       "      <td>0.250475</td>\n",
       "      <td>22.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22.0</td>\n",
       "      <td>-1.946525</td>\n",
       "      <td>-0.044901</td>\n",
       "      <td>-0.405570</td>\n",
       "      <td>-1.013057</td>\n",
       "      <td>2.941968</td>\n",
       "      <td>2.955053</td>\n",
       "      <td>-0.063063</td>\n",
       "      <td>0.855546</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579526</td>\n",
       "      <td>-0.799229</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>0.983421</td>\n",
       "      <td>0.321201</td>\n",
       "      <td>0.149650</td>\n",
       "      <td>0.707519</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22.0</td>\n",
       "      <td>-2.074295</td>\n",
       "      <td>-0.121482</td>\n",
       "      <td>1.322021</td>\n",
       "      <td>0.410008</td>\n",
       "      <td>0.295198</td>\n",
       "      <td>-0.959537</td>\n",
       "      <td>0.543985</td>\n",
       "      <td>-0.104627</td>\n",
       "      <td>0.475664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403639</td>\n",
       "      <td>-0.227404</td>\n",
       "      <td>0.742435</td>\n",
       "      <td>0.398535</td>\n",
       "      <td>0.249212</td>\n",
       "      <td>0.274404</td>\n",
       "      <td>0.359969</td>\n",
       "      <td>0.243232</td>\n",
       "      <td>26.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.173285</td>\n",
       "      <td>0.353498</td>\n",
       "      <td>0.283905</td>\n",
       "      <td>1.133563</td>\n",
       "      <td>-0.172577</td>\n",
       "      <td>-0.916054</td>\n",
       "      <td>0.369025</td>\n",
       "      <td>-0.327260</td>\n",
       "      <td>-0.246651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067003</td>\n",
       "      <td>0.227812</td>\n",
       "      <td>-0.150487</td>\n",
       "      <td>0.435045</td>\n",
       "      <td>0.724825</td>\n",
       "      <td>-0.337082</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>41.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.322707</td>\n",
       "      <td>-0.174041</td>\n",
       "      <td>0.434555</td>\n",
       "      <td>0.576038</td>\n",
       "      <td>-0.836758</td>\n",
       "      <td>-0.831083</td>\n",
       "      <td>-0.264905</td>\n",
       "      <td>-0.220982</td>\n",
       "      <td>-1.071425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>-0.323357</td>\n",
       "      <td>-0.037710</td>\n",
       "      <td>0.347151</td>\n",
       "      <td>0.559639</td>\n",
       "      <td>-0.280158</td>\n",
       "      <td>0.042335</td>\n",
       "      <td>0.028822</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23.0</td>\n",
       "      <td>-0.414289</td>\n",
       "      <td>0.905437</td>\n",
       "      <td>1.727453</td>\n",
       "      <td>1.473471</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>-0.200331</td>\n",
       "      <td>0.740228</td>\n",
       "      <td>-0.029247</td>\n",
       "      <td>-0.593392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077237</td>\n",
       "      <td>0.457331</td>\n",
       "      <td>-0.038500</td>\n",
       "      <td>0.642522</td>\n",
       "      <td>-0.183891</td>\n",
       "      <td>-0.277464</td>\n",
       "      <td>0.182687</td>\n",
       "      <td>0.152665</td>\n",
       "      <td>33.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.059387</td>\n",
       "      <td>-0.175319</td>\n",
       "      <td>1.266130</td>\n",
       "      <td>1.186110</td>\n",
       "      <td>-0.786002</td>\n",
       "      <td>0.578435</td>\n",
       "      <td>-0.767084</td>\n",
       "      <td>0.401046</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>0.213734</td>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.294638</td>\n",
       "      <td>-0.395070</td>\n",
       "      <td>0.081461</td>\n",
       "      <td>0.024220</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.237429</td>\n",
       "      <td>0.061043</td>\n",
       "      <td>0.380526</td>\n",
       "      <td>0.761564</td>\n",
       "      <td>-0.359771</td>\n",
       "      <td>-0.494084</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>-0.133862</td>\n",
       "      <td>0.438810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245682</td>\n",
       "      <td>-0.530900</td>\n",
       "      <td>-0.044265</td>\n",
       "      <td>0.079168</td>\n",
       "      <td>0.509136</td>\n",
       "      <td>0.288858</td>\n",
       "      <td>-0.022705</td>\n",
       "      <td>0.011836</td>\n",
       "      <td>17.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.114009</td>\n",
       "      <td>0.085546</td>\n",
       "      <td>0.493702</td>\n",
       "      <td>1.335760</td>\n",
       "      <td>-0.300189</td>\n",
       "      <td>-0.010754</td>\n",
       "      <td>-0.118760</td>\n",
       "      <td>0.188617</td>\n",
       "      <td>0.205687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053234</td>\n",
       "      <td>-0.004760</td>\n",
       "      <td>-0.031470</td>\n",
       "      <td>0.198054</td>\n",
       "      <td>0.565007</td>\n",
       "      <td>-0.337718</td>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.529912</td>\n",
       "      <td>0.873892</td>\n",
       "      <td>1.347247</td>\n",
       "      <td>0.145457</td>\n",
       "      <td>0.414209</td>\n",
       "      <td>0.100223</td>\n",
       "      <td>0.711206</td>\n",
       "      <td>0.176066</td>\n",
       "      <td>-0.286717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046949</td>\n",
       "      <td>0.208105</td>\n",
       "      <td>-0.185548</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.098816</td>\n",
       "      <td>-0.552904</td>\n",
       "      <td>-0.073288</td>\n",
       "      <td>0.023307</td>\n",
       "      <td>6.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.535388</td>\n",
       "      <td>0.865268</td>\n",
       "      <td>1.351076</td>\n",
       "      <td>0.147575</td>\n",
       "      <td>0.433680</td>\n",
       "      <td>0.086983</td>\n",
       "      <td>0.693039</td>\n",
       "      <td>0.179742</td>\n",
       "      <td>-0.285642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.206537</td>\n",
       "      <td>-0.187108</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.098117</td>\n",
       "      <td>-0.553471</td>\n",
       "      <td>-0.078306</td>\n",
       "      <td>0.025427</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.452187</td>\n",
       "      <td>1.765124</td>\n",
       "      <td>0.611669</td>\n",
       "      <td>1.176825</td>\n",
       "      <td>-0.445980</td>\n",
       "      <td>0.246826</td>\n",
       "      <td>-0.257566</td>\n",
       "      <td>1.092472</td>\n",
       "      <td>-0.607524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082280</td>\n",
       "      <td>0.325782</td>\n",
       "      <td>-0.069107</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>-0.044668</td>\n",
       "      <td>-0.243441</td>\n",
       "      <td>0.149180</td>\n",
       "      <td>0.120557</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1.249055</td>\n",
       "      <td>-0.624727</td>\n",
       "      <td>-0.710589</td>\n",
       "      <td>-0.991600</td>\n",
       "      <td>1.429973</td>\n",
       "      <td>3.692977</td>\n",
       "      <td>-1.090209</td>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.850149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006293</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>-0.129463</td>\n",
       "      <td>1.112970</td>\n",
       "      <td>0.500382</td>\n",
       "      <td>1.196549</td>\n",
       "      <td>-0.048220</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>29.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32.0</td>\n",
       "      <td>-2.008872</td>\n",
       "      <td>2.198527</td>\n",
       "      <td>0.144242</td>\n",
       "      <td>1.159432</td>\n",
       "      <td>-0.815174</td>\n",
       "      <td>0.182288</td>\n",
       "      <td>-0.617108</td>\n",
       "      <td>1.530817</td>\n",
       "      <td>-0.586832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094917</td>\n",
       "      <td>0.294983</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>0.034211</td>\n",
       "      <td>-0.236141</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.117986</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33.0</td>\n",
       "      <td>-0.607877</td>\n",
       "      <td>1.031345</td>\n",
       "      <td>1.740450</td>\n",
       "      <td>1.232106</td>\n",
       "      <td>0.418592</td>\n",
       "      <td>0.119168</td>\n",
       "      <td>0.850893</td>\n",
       "      <td>-0.176267</td>\n",
       "      <td>-0.243501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087329</td>\n",
       "      <td>0.258315</td>\n",
       "      <td>-0.264775</td>\n",
       "      <td>0.118282</td>\n",
       "      <td>0.173508</td>\n",
       "      <td>-0.217041</td>\n",
       "      <td>0.094312</td>\n",
       "      <td>-0.033041</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199334</th>\n",
       "      <td>172758.0</td>\n",
       "      <td>-0.546378</td>\n",
       "      <td>1.433992</td>\n",
       "      <td>-0.313252</td>\n",
       "      <td>0.926044</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>-0.565669</td>\n",
       "      <td>1.066075</td>\n",
       "      <td>0.269799</td>\n",
       "      <td>-1.099446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228159</td>\n",
       "      <td>0.778817</td>\n",
       "      <td>-0.175451</td>\n",
       "      <td>0.030397</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>-0.395994</td>\n",
       "      <td>0.301655</td>\n",
       "      <td>0.173585</td>\n",
       "      <td>36.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199335</th>\n",
       "      <td>172759.0</td>\n",
       "      <td>-1.570301</td>\n",
       "      <td>1.880590</td>\n",
       "      <td>-0.559004</td>\n",
       "      <td>-1.003017</td>\n",
       "      <td>-0.326096</td>\n",
       "      <td>-0.417224</td>\n",
       "      <td>-0.477372</td>\n",
       "      <td>1.122221</td>\n",
       "      <td>0.613186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266727</td>\n",
       "      <td>-0.854690</td>\n",
       "      <td>-0.034251</td>\n",
       "      <td>-1.154617</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>-0.233612</td>\n",
       "      <td>-0.427107</td>\n",
       "      <td>-0.255729</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199336</th>\n",
       "      <td>172759.0</td>\n",
       "      <td>1.894910</td>\n",
       "      <td>-0.427906</td>\n",
       "      <td>-2.184267</td>\n",
       "      <td>0.159979</td>\n",
       "      <td>0.587740</td>\n",
       "      <td>-0.557966</td>\n",
       "      <td>0.510524</td>\n",
       "      <td>-0.281590</td>\n",
       "      <td>0.785446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122775</td>\n",
       "      <td>0.202373</td>\n",
       "      <td>-0.125329</td>\n",
       "      <td>0.017737</td>\n",
       "      <td>0.396759</td>\n",
       "      <td>-0.515948</td>\n",
       "      <td>-0.039485</td>\n",
       "      <td>-0.040441</td>\n",
       "      <td>124.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199337</th>\n",
       "      <td>172760.0</td>\n",
       "      <td>-6.713826</td>\n",
       "      <td>2.369104</td>\n",
       "      <td>-3.534768</td>\n",
       "      <td>-0.368259</td>\n",
       "      <td>-1.721030</td>\n",
       "      <td>-1.322808</td>\n",
       "      <td>-0.190876</td>\n",
       "      <td>0.891149</td>\n",
       "      <td>2.068476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131247</td>\n",
       "      <td>0.994246</td>\n",
       "      <td>-0.002140</td>\n",
       "      <td>-0.072979</td>\n",
       "      <td>-0.167159</td>\n",
       "      <td>-0.387271</td>\n",
       "      <td>-2.616341</td>\n",
       "      <td>0.474584</td>\n",
       "      <td>19.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199338</th>\n",
       "      <td>172761.0</td>\n",
       "      <td>-0.066136</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>-0.918564</td>\n",
       "      <td>-0.923242</td>\n",
       "      <td>1.256522</td>\n",
       "      <td>0.047446</td>\n",
       "      <td>0.705820</td>\n",
       "      <td>0.232330</td>\n",
       "      <td>0.213157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397210</td>\n",
       "      <td>-1.038099</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>-0.512741</td>\n",
       "      <td>-0.349721</td>\n",
       "      <td>0.168693</td>\n",
       "      <td>0.316674</td>\n",
       "      <td>0.120065</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199339</th>\n",
       "      <td>172762.0</td>\n",
       "      <td>1.955547</td>\n",
       "      <td>-0.724606</td>\n",
       "      <td>-1.706511</td>\n",
       "      <td>-0.611145</td>\n",
       "      <td>1.710907</td>\n",
       "      <td>3.914215</td>\n",
       "      <td>-1.248690</td>\n",
       "      <td>1.054133</td>\n",
       "      <td>1.314064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>0.726775</td>\n",
       "      <td>-0.061219</td>\n",
       "      <td>-0.192666</td>\n",
       "      <td>0.060347</td>\n",
       "      <td>-0.042323</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199340</th>\n",
       "      <td>172762.0</td>\n",
       "      <td>-1.143909</td>\n",
       "      <td>1.780421</td>\n",
       "      <td>-0.454124</td>\n",
       "      <td>-0.814393</td>\n",
       "      <td>0.514689</td>\n",
       "      <td>-0.926634</td>\n",
       "      <td>1.196374</td>\n",
       "      <td>-0.349255</td>\n",
       "      <td>0.863877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173511</td>\n",
       "      <td>1.491648</td>\n",
       "      <td>-0.156954</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>-0.630124</td>\n",
       "      <td>-0.301970</td>\n",
       "      <td>0.425852</td>\n",
       "      <td>-0.109068</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199341</th>\n",
       "      <td>172764.0</td>\n",
       "      <td>-0.764523</td>\n",
       "      <td>0.588379</td>\n",
       "      <td>-0.907599</td>\n",
       "      <td>-0.418847</td>\n",
       "      <td>0.901528</td>\n",
       "      <td>-0.760802</td>\n",
       "      <td>0.758545</td>\n",
       "      <td>0.414698</td>\n",
       "      <td>-0.730854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>-0.431876</td>\n",
       "      <td>0.141759</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>-0.200998</td>\n",
       "      <td>0.267337</td>\n",
       "      <td>-0.152951</td>\n",
       "      <td>-0.065285</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199342</th>\n",
       "      <td>172766.0</td>\n",
       "      <td>1.975178</td>\n",
       "      <td>-0.616244</td>\n",
       "      <td>-2.628295</td>\n",
       "      <td>-0.406246</td>\n",
       "      <td>2.327804</td>\n",
       "      <td>3.664740</td>\n",
       "      <td>-0.533297</td>\n",
       "      <td>0.842937</td>\n",
       "      <td>1.128798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086043</td>\n",
       "      <td>0.543613</td>\n",
       "      <td>-0.032129</td>\n",
       "      <td>0.768379</td>\n",
       "      <td>0.477688</td>\n",
       "      <td>-0.031833</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>-0.066542</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199343</th>\n",
       "      <td>172766.0</td>\n",
       "      <td>-1.139015</td>\n",
       "      <td>-0.155510</td>\n",
       "      <td>1.894478</td>\n",
       "      <td>-1.138957</td>\n",
       "      <td>1.451777</td>\n",
       "      <td>0.093598</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.092211</td>\n",
       "      <td>-0.062621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191027</td>\n",
       "      <td>-0.631658</td>\n",
       "      <td>-0.147249</td>\n",
       "      <td>0.212931</td>\n",
       "      <td>0.354257</td>\n",
       "      <td>-0.241068</td>\n",
       "      <td>-0.161717</td>\n",
       "      <td>-0.149188</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199344</th>\n",
       "      <td>172767.0</td>\n",
       "      <td>-0.268061</td>\n",
       "      <td>2.540315</td>\n",
       "      <td>-1.400915</td>\n",
       "      <td>4.846661</td>\n",
       "      <td>0.639105</td>\n",
       "      <td>0.186479</td>\n",
       "      <td>-0.045911</td>\n",
       "      <td>0.936448</td>\n",
       "      <td>-2.419986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263889</td>\n",
       "      <td>-0.857904</td>\n",
       "      <td>0.235172</td>\n",
       "      <td>-0.681794</td>\n",
       "      <td>-0.668894</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>-0.066751</td>\n",
       "      <td>-0.072447</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199345</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-1.796092</td>\n",
       "      <td>1.929178</td>\n",
       "      <td>-2.828417</td>\n",
       "      <td>-1.689844</td>\n",
       "      <td>2.199572</td>\n",
       "      <td>3.123732</td>\n",
       "      <td>-0.270714</td>\n",
       "      <td>1.657495</td>\n",
       "      <td>0.465804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271170</td>\n",
       "      <td>1.145750</td>\n",
       "      <td>0.084783</td>\n",
       "      <td>0.721269</td>\n",
       "      <td>-0.529906</td>\n",
       "      <td>-0.240117</td>\n",
       "      <td>0.129126</td>\n",
       "      <td>-0.080620</td>\n",
       "      <td>11.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199346</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-0.669662</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>-1.543167</td>\n",
       "      <td>-1.560729</td>\n",
       "      <td>2.833960</td>\n",
       "      <td>3.240843</td>\n",
       "      <td>0.181576</td>\n",
       "      <td>1.282746</td>\n",
       "      <td>-0.893890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183856</td>\n",
       "      <td>0.202670</td>\n",
       "      <td>-0.373023</td>\n",
       "      <td>0.651122</td>\n",
       "      <td>1.073823</td>\n",
       "      <td>0.844590</td>\n",
       "      <td>-0.286676</td>\n",
       "      <td>-0.187719</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199347</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-2.076175</td>\n",
       "      <td>2.142238</td>\n",
       "      <td>-2.522704</td>\n",
       "      <td>-1.888063</td>\n",
       "      <td>1.982785</td>\n",
       "      <td>3.732950</td>\n",
       "      <td>-1.217430</td>\n",
       "      <td>-0.536644</td>\n",
       "      <td>0.272867</td>\n",
       "      <td>...</td>\n",
       "      <td>2.016666</td>\n",
       "      <td>-1.588269</td>\n",
       "      <td>0.588482</td>\n",
       "      <td>0.632444</td>\n",
       "      <td>-0.201064</td>\n",
       "      <td>0.199251</td>\n",
       "      <td>0.438657</td>\n",
       "      <td>0.172923</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199348</th>\n",
       "      <td>172769.0</td>\n",
       "      <td>-1.029719</td>\n",
       "      <td>-1.110670</td>\n",
       "      <td>-0.636179</td>\n",
       "      <td>-0.840816</td>\n",
       "      <td>2.424360</td>\n",
       "      <td>-2.956733</td>\n",
       "      <td>0.283610</td>\n",
       "      <td>-0.332656</td>\n",
       "      <td>-0.247488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353722</td>\n",
       "      <td>0.488487</td>\n",
       "      <td>0.293632</td>\n",
       "      <td>0.107812</td>\n",
       "      <td>-0.935586</td>\n",
       "      <td>1.138216</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.255347</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199349</th>\n",
       "      <td>172770.0</td>\n",
       "      <td>2.007418</td>\n",
       "      <td>-0.280235</td>\n",
       "      <td>-0.208113</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>-0.715798</td>\n",
       "      <td>-0.751373</td>\n",
       "      <td>-0.458972</td>\n",
       "      <td>-0.140140</td>\n",
       "      <td>0.959971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208260</td>\n",
       "      <td>-0.430347</td>\n",
       "      <td>0.416765</td>\n",
       "      <td>0.064819</td>\n",
       "      <td>-0.608337</td>\n",
       "      <td>0.268436</td>\n",
       "      <td>-0.028069</td>\n",
       "      <td>-0.041367</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199350</th>\n",
       "      <td>172770.0</td>\n",
       "      <td>-0.446951</td>\n",
       "      <td>1.302212</td>\n",
       "      <td>-0.168583</td>\n",
       "      <td>0.981577</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>-0.605641</td>\n",
       "      <td>1.253430</td>\n",
       "      <td>-1.042610</td>\n",
       "      <td>-0.417116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.305268</td>\n",
       "      <td>-0.148093</td>\n",
       "      <td>-0.038712</td>\n",
       "      <td>0.010209</td>\n",
       "      <td>-0.362666</td>\n",
       "      <td>0.503092</td>\n",
       "      <td>0.229921</td>\n",
       "      <td>60.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199351</th>\n",
       "      <td>172771.0</td>\n",
       "      <td>-0.515513</td>\n",
       "      <td>0.971950</td>\n",
       "      <td>-1.014580</td>\n",
       "      <td>-0.677037</td>\n",
       "      <td>0.912430</td>\n",
       "      <td>-0.316187</td>\n",
       "      <td>0.396137</td>\n",
       "      <td>0.532364</td>\n",
       "      <td>-0.224606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280302</td>\n",
       "      <td>-0.849919</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>-0.376379</td>\n",
       "      <td>0.128660</td>\n",
       "      <td>-0.015205</td>\n",
       "      <td>-0.021486</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199352</th>\n",
       "      <td>172774.0</td>\n",
       "      <td>-0.863506</td>\n",
       "      <td>0.874701</td>\n",
       "      <td>0.420358</td>\n",
       "      <td>-0.530365</td>\n",
       "      <td>0.356561</td>\n",
       "      <td>-1.046238</td>\n",
       "      <td>0.757051</td>\n",
       "      <td>0.230473</td>\n",
       "      <td>-0.506856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108846</td>\n",
       "      <td>-0.480820</td>\n",
       "      <td>-0.074513</td>\n",
       "      <td>-0.003988</td>\n",
       "      <td>-0.113149</td>\n",
       "      <td>0.280378</td>\n",
       "      <td>-0.077310</td>\n",
       "      <td>0.023079</td>\n",
       "      <td>20.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199353</th>\n",
       "      <td>172774.0</td>\n",
       "      <td>-0.724123</td>\n",
       "      <td>1.485216</td>\n",
       "      <td>-1.132218</td>\n",
       "      <td>-0.607190</td>\n",
       "      <td>0.709499</td>\n",
       "      <td>-0.482638</td>\n",
       "      <td>0.548393</td>\n",
       "      <td>0.343003</td>\n",
       "      <td>-0.226323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414621</td>\n",
       "      <td>1.307511</td>\n",
       "      <td>-0.059545</td>\n",
       "      <td>0.242669</td>\n",
       "      <td>-0.665424</td>\n",
       "      <td>-0.269869</td>\n",
       "      <td>-0.170579</td>\n",
       "      <td>-0.030692</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199354</th>\n",
       "      <td>172775.0</td>\n",
       "      <td>1.971002</td>\n",
       "      <td>-0.699067</td>\n",
       "      <td>-1.697541</td>\n",
       "      <td>-0.617643</td>\n",
       "      <td>1.718797</td>\n",
       "      <td>3.911336</td>\n",
       "      <td>-1.259306</td>\n",
       "      <td>1.056209</td>\n",
       "      <td>1.315006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188758</td>\n",
       "      <td>0.694418</td>\n",
       "      <td>0.163002</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>-0.058282</td>\n",
       "      <td>-0.191813</td>\n",
       "      <td>0.061858</td>\n",
       "      <td>-0.043716</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199355</th>\n",
       "      <td>172777.0</td>\n",
       "      <td>-1.266580</td>\n",
       "      <td>-0.400461</td>\n",
       "      <td>0.956221</td>\n",
       "      <td>-0.723919</td>\n",
       "      <td>1.531993</td>\n",
       "      <td>-1.788600</td>\n",
       "      <td>0.314741</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157831</td>\n",
       "      <td>-0.883365</td>\n",
       "      <td>0.088485</td>\n",
       "      <td>-0.076790</td>\n",
       "      <td>-0.095833</td>\n",
       "      <td>0.132720</td>\n",
       "      <td>-0.028468</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199356</th>\n",
       "      <td>172778.0</td>\n",
       "      <td>-12.516732</td>\n",
       "      <td>10.187818</td>\n",
       "      <td>-8.476671</td>\n",
       "      <td>-2.510473</td>\n",
       "      <td>-4.586669</td>\n",
       "      <td>-1.394465</td>\n",
       "      <td>-3.632516</td>\n",
       "      <td>5.498583</td>\n",
       "      <td>4.893089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.944759</td>\n",
       "      <td>-1.565026</td>\n",
       "      <td>0.890675</td>\n",
       "      <td>-1.253276</td>\n",
       "      <td>1.786717</td>\n",
       "      <td>0.320763</td>\n",
       "      <td>2.090712</td>\n",
       "      <td>1.232864</td>\n",
       "      <td>9.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199357</th>\n",
       "      <td>172780.0</td>\n",
       "      <td>1.884849</td>\n",
       "      <td>-0.143540</td>\n",
       "      <td>-0.999943</td>\n",
       "      <td>1.506772</td>\n",
       "      <td>-0.035300</td>\n",
       "      <td>-0.613638</td>\n",
       "      <td>0.190241</td>\n",
       "      <td>-0.249058</td>\n",
       "      <td>0.666458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.634646</td>\n",
       "      <td>-0.042114</td>\n",
       "      <td>-0.053206</td>\n",
       "      <td>0.316403</td>\n",
       "      <td>-0.461441</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>-0.041068</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199358</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199359</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>24.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199360</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199361</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199362</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199363</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199364 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "4            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n",
       "5            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n",
       "6            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n",
       "7           10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n",
       "8           11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n",
       "9           12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n",
       "10          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n",
       "11          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n",
       "12          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n",
       "13          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n",
       "14          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n",
       "15          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n",
       "16          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n",
       "17          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n",
       "18          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n",
       "19          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n",
       "20          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n",
       "21          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n",
       "22          24.0   1.237429   0.061043  0.380526  0.761564 -0.359771   \n",
       "23          25.0   1.114009   0.085546  0.493702  1.335760 -0.300189   \n",
       "24          26.0  -0.529912   0.873892  1.347247  0.145457  0.414209   \n",
       "25          26.0  -0.535388   0.865268  1.351076  0.147575  0.433680   \n",
       "26          27.0  -1.452187   1.765124  0.611669  1.176825 -0.445980   \n",
       "27          32.0   1.249055  -0.624727 -0.710589 -0.991600  1.429973   \n",
       "28          32.0  -2.008872   2.198527  0.144242  1.159432 -0.815174   \n",
       "29          33.0  -0.607877   1.031345  1.740450  1.232106  0.418592   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "199334  172758.0  -0.546378   1.433992 -0.313252  0.926044  0.522388   \n",
       "199335  172759.0  -1.570301   1.880590 -0.559004 -1.003017 -0.326096   \n",
       "199336  172759.0   1.894910  -0.427906 -2.184267  0.159979  0.587740   \n",
       "199337  172760.0  -6.713826   2.369104 -3.534768 -0.368259 -1.721030   \n",
       "199338  172761.0  -0.066136   0.996296 -0.918564 -0.923242  1.256522   \n",
       "199339  172762.0   1.955547  -0.724606 -1.706511 -0.611145  1.710907   \n",
       "199340  172762.0  -1.143909   1.780421 -0.454124 -0.814393  0.514689   \n",
       "199341  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n",
       "199342  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n",
       "199343  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n",
       "199344  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
       "199345  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
       "199346  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
       "199347  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
       "199348  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
       "199349  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
       "199350  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
       "199351  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
       "199352  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
       "199353  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
       "199354  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
       "199355  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
       "199356  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
       "199357  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
       "199358  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
       "199359  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
       "199360  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "199361  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "199362  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "199363  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "4       0.272708 -0.005159  0.081213  0.464960  ... -0.167716 -0.270710   \n",
       "5       3.721818  0.370145  0.851084 -0.392048  ... -0.073425 -0.268092   \n",
       "6      -0.246761  0.651583  0.069539 -0.736727  ... -0.246914 -0.633753   \n",
       "7      -0.753230 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285   \n",
       "8       0.337544 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412   \n",
       "9       0.807596 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182   \n",
       "10     -0.077850 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650   \n",
       "11     -0.127867  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638   \n",
       "12     -0.720961 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074   \n",
       "13      1.309109 -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955   \n",
       "14      0.089474  0.241147  0.138082 -0.989162  ...  0.018702 -0.061972   \n",
       "15     -0.150116 -0.946365 -1.617935  1.544071  ...  1.650180  0.200454   \n",
       "16      2.955053 -0.063063  0.855546  0.049967  ... -0.579526 -0.799229   \n",
       "17     -0.959537  0.543985 -0.104627  0.475664  ... -0.403639 -0.227404   \n",
       "18     -0.916054  0.369025 -0.327260 -0.246651  ...  0.067003  0.227812   \n",
       "19     -0.831083 -0.264905 -0.220982 -1.071425  ... -0.284376 -0.323357   \n",
       "20     -0.200331  0.740228 -0.029247 -0.593392  ...  0.077237  0.457331   \n",
       "21      0.578435 -0.767084  0.401046  0.699500  ...  0.013676  0.213734   \n",
       "22     -0.494084  0.006494 -0.133862  0.438810  ... -0.245682 -0.530900   \n",
       "23     -0.010754 -0.118760  0.188617  0.205687  ... -0.053234 -0.004760   \n",
       "24      0.100223  0.711206  0.176066 -0.286717  ...  0.046949  0.208105   \n",
       "25      0.086983  0.693039  0.179742 -0.285642  ...  0.049526  0.206537   \n",
       "26      0.246826 -0.257566  1.092472 -0.607524  ...  0.082280  0.325782   \n",
       "27      3.692977 -1.090209  0.967291  0.850149  ... -0.006293  0.009200   \n",
       "28      0.182288 -0.617108  1.530817 -0.586832  ...  0.094917  0.294983   \n",
       "29      0.119168  0.850893 -0.176267 -0.243501  ... -0.087329  0.258315   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "199334 -0.565669  1.066075  0.269799 -1.099446  ...  0.228159  0.778817   \n",
       "199335 -0.417224 -0.477372  1.122221  0.613186  ... -0.266727 -0.854690   \n",
       "199336 -0.557966  0.510524 -0.281590  0.785446  ...  0.122775  0.202373   \n",
       "199337 -1.322808 -0.190876  0.891149  2.068476  ...  0.131247  0.994246   \n",
       "199338  0.047446  0.705820  0.232330  0.213157  ... -0.397210 -1.038099   \n",
       "199339  3.914215 -1.248690  1.054133  1.314064  ...  0.193605  0.690196   \n",
       "199340 -0.926634  1.196374 -0.349255  0.863877  ...  0.173511  1.491648   \n",
       "199341 -0.760802  0.758545  0.414698 -0.730854  ...  0.003530 -0.431876   \n",
       "199342  3.664740 -0.533297  0.842937  1.128798  ...  0.086043  0.543613   \n",
       "199343  0.093598  0.191353  0.092211 -0.062621  ... -0.191027 -0.631658   \n",
       "199344  0.186479 -0.045911  0.936448 -2.419986  ... -0.263889 -0.857904   \n",
       "199345  3.123732 -0.270714  1.657495  0.465804  ...  0.271170  1.145750   \n",
       "199346  3.240843  0.181576  1.282746 -0.893890  ...  0.183856  0.202670   \n",
       "199347  3.732950 -1.217430 -0.536644  0.272867  ...  2.016666 -1.588269   \n",
       "199348 -2.956733  0.283610 -0.332656 -0.247488  ...  0.353722  0.488487   \n",
       "199349 -0.751373 -0.458972 -0.140140  0.959971  ... -0.208260 -0.430347   \n",
       "199350 -0.605641  1.253430 -1.042610 -0.417116  ...  0.851800  0.305268   \n",
       "199351 -0.316187  0.396137  0.532364 -0.224606  ... -0.280302 -0.849919   \n",
       "199352 -1.046238  0.757051  0.230473 -0.506856  ... -0.108846 -0.480820   \n",
       "199353 -0.482638  0.548393  0.343003 -0.226323  ...  0.414621  1.307511   \n",
       "199354  3.911336 -1.259306  1.056209  1.315006  ...  0.188758  0.694418   \n",
       "199355 -1.788600  0.314741  0.004704  0.013857  ... -0.157831 -0.883365   \n",
       "199356 -1.394465 -3.632516  5.498583  4.893089  ... -0.944759 -1.565026   \n",
       "199357 -0.613638  0.190241 -0.249058  0.666458  ...  0.144008  0.634646   \n",
       "199358 -1.343668  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376   \n",
       "199359 -1.014307  0.427126  0.121340 -0.285670  ...  0.099936  0.337120   \n",
       "199360 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "199361  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "199362  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "199363 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "4      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n",
       "5      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n",
       "6      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n",
       "7       0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n",
       "8      -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n",
       "9       1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n",
       "10     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n",
       "11     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n",
       "12      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n",
       "13     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n",
       "14     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n",
       "15     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n",
       "16      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n",
       "17      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n",
       "18     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n",
       "19     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n",
       "20     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n",
       "21      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n",
       "22     -0.044265  0.079168  0.509136  0.288858 -0.022705  0.011836   17.28   \n",
       "23     -0.031470  0.198054  0.565007 -0.337718  0.029057  0.004453    4.45   \n",
       "24     -0.185548  0.001031  0.098816 -0.552904 -0.073288  0.023307    6.14   \n",
       "25     -0.187108  0.000753  0.098117 -0.553471 -0.078306  0.025427    1.77   \n",
       "26     -0.069107  0.020962 -0.044668 -0.243441  0.149180  0.120557    1.80   \n",
       "27     -0.129463  1.112970  0.500382  1.196549 -0.048220  0.005094   29.89   \n",
       "28      0.011081  0.015249  0.034211 -0.236141  0.128291  0.117986    2.35   \n",
       "29     -0.264775  0.118282  0.173508 -0.217041  0.094312 -0.033041   14.80   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "199334 -0.175451  0.030397  0.018381 -0.395994  0.301655  0.173585   36.51   \n",
       "199335 -0.034251 -1.154617  0.036740 -0.233612 -0.427107 -0.255729    0.77   \n",
       "199336 -0.125329  0.017737  0.396759 -0.515948 -0.039485 -0.040441  124.99   \n",
       "199337 -0.002140 -0.072979 -0.167159 -0.387271 -2.616341  0.474584   19.59   \n",
       "199338  0.002120 -0.512741 -0.349721  0.168693  0.316674  0.120065    3.57   \n",
       "199339  0.155951  0.726775 -0.061219 -0.192666  0.060347 -0.042323   12.99   \n",
       "199340 -0.156954  0.053810 -0.630124 -0.301970  0.425852 -0.109068    7.22   \n",
       "199341  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n",
       "199342 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n",
       "199343 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n",
       "199344  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n",
       "199345  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n",
       "199346 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n",
       "199347  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n",
       "199348  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n",
       "199349  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n",
       "199350 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n",
       "199351  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n",
       "199352 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n",
       "199353 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n",
       "199354  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n",
       "199355  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n",
       "199356  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n",
       "199357 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n",
       "199358  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n",
       "199359  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n",
       "199360  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "199361  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "199362 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "199363  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  \n",
       "10          0  \n",
       "11          0  \n",
       "12          0  \n",
       "13          0  \n",
       "14          0  \n",
       "15          0  \n",
       "16          0  \n",
       "17          0  \n",
       "18          0  \n",
       "19          0  \n",
       "20          0  \n",
       "21          0  \n",
       "22          0  \n",
       "23          0  \n",
       "24          0  \n",
       "25          0  \n",
       "26          0  \n",
       "27          0  \n",
       "28          0  \n",
       "29          0  \n",
       "...       ...  \n",
       "199334      0  \n",
       "199335      0  \n",
       "199336      0  \n",
       "199337      0  \n",
       "199338      0  \n",
       "199339      0  \n",
       "199340      0  \n",
       "199341      0  \n",
       "199342      0  \n",
       "199343      0  \n",
       "199344      0  \n",
       "199345      0  \n",
       "199346      0  \n",
       "199347      0  \n",
       "199348      0  \n",
       "199349      0  \n",
       "199350      0  \n",
       "199351      0  \n",
       "199352      0  \n",
       "199353      0  \n",
       "199354      0  \n",
       "199355      0  \n",
       "199356      0  \n",
       "199357      0  \n",
       "199358      0  \n",
       "199359      0  \n",
       "199360      0  \n",
       "199361      0  \n",
       "199362      0  \n",
       "199363      0  \n",
       "\n",
       "[199364 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"project_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원하는 feature 뽑아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectFeature(data, feature_list):\n",
    "    \n",
    "    X = data[feature_list].values\n",
    "    y = data['Class'].values\n",
    "    \n",
    "    return X,y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 케이스 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1 Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def CalScore(true,predict):\n",
    "\n",
    "    \n",
    "    f1Score = f1_score(true, predict, average='binary')\n",
    "    print(f1Score)\n",
    "    '''\n",
    "    f1Score = f1_score(true, predict, average='macro')\n",
    "    print(f1Score)\n",
    "    f1Score = f1_score(true, predict, average='micro')\n",
    "    print(f1Score)\n",
    "    f1Score = f1_score(true, predict, average='weighted')\n",
    "    print(f1Score)\n",
    "    f1Score = f1_score(true, predict, average=None)\n",
    "    print(f1Score)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f beta Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def CalBetaScore(true,predict):\n",
    "    fBetaScore = fbeta_score(true, predict, average='binary', beta=1.5) #recall을 더 높은 비중으로\n",
    "    print(fBetaScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 time에 따른 feature값을 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/feature_time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. feature 전체 \n",
    "2. feature들 중 abnormal이 극명하게 들어나는 V11,V12,V14,V16,V17을 선택 (feature 5개)\n",
    "3. Class값과 Relation이 높은 V1, V2, V3, V4, V5, V7, V9, V10, V11, V12, V14, V16, V17, V18을 선택 (feature 14개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opp06\\Anaconda3\\envs\\MLProject\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.6795454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opp06\\Anaconda3\\envs\\MLProject\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8270676691729324\n",
      "0.7839912280701753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opp06\\Anaconda3\\envs\\MLProject\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6495726495726496\n",
      "0.5825471698113207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#feature 전체\n",
    "feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "predict = svm.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)\n",
    "\n",
    "#좋은 feature 선별\n",
    "feature_sub = ['V11','V12','V14','V16','V17'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_sub);\n",
    "test_X , test_y = SelectFeature(test_set,feature_sub);\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "predict = svm.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)\n",
    "\n",
    "#Correlation이 높은 feature 선별\n",
    "feature_sub2 = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_sub2);\n",
    "test_X , test_y = SelectFeature(test_set,feature_sub2);\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "predict = svm.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "기본 SVM을 돌려도 상당히 차이나는 수치가 나옴을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['V11','V12','V14','V16','V17'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_list);\n",
    "test_X , test_y = SelectFeature(test_set,feature_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 Classification 알고리즘 비교 \n",
    "- Logistic Regression \n",
    "- SVM \n",
    "- Decision Tree \n",
    "- Random Forest \n",
    "- Gradient Boosting Classifier \n",
    "- MLP \n",
    "- k-NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opp06\\Anaconda3\\envs\\MLProject\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6451612903225806\n",
      "0.5936073059360731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(train_X, train_y)\n",
    "\n",
    "#predict = logit.predict(train_X)\n",
    "#CalScore(train_y,predict)\n",
    "\n",
    "predict = logit.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opp06\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8270676691729324\n",
      "0.7839912280701753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "#predict = svm.predict(train_X)\n",
    "#CalScore(train_y,predict)\n",
    "\n",
    "predict = svm.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7297297297297298\n",
      "0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree.fit(train_X, train_y)\n",
    "\n",
    "predict = decisionTree.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842857142857143\n",
      "0.8159574468085106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = 0, n_estimators = 100,criterion = 'entropy')\n",
    "rfc.fit(train_X, train_y)\n",
    "\n",
    "#predict = rfc.predict(train_X)\n",
    "#CalScore(train_y,predict)\n",
    "\n",
    "predict = rfc.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194244604316548\n",
      "0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(train_X, train_y)\n",
    "\n",
    "predict = gbc.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7943262411347517\n",
      "0.771186440677966\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(train_X, train_y)\n",
    "\n",
    "predict = xgb_model.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786206896551724\n",
      "0.771875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)  \n",
    "mlp.fit(train_X, train_y)  \n",
    "\n",
    "predict = mlp.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8201438848920864\n",
      "0.7916666666666669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_X, train_y)\n",
    "\n",
    "predict = knn.predict(test_X)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습값 평가\n",
    "<img src=\"img/model_select.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm 파라미터 튜닝 with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'C': 10, 'gamma': 0.1}\n",
      "Best Estimators:\n",
      " SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001,0.01,0.1, 1, 10, 100,1000], 'gamma': ['auto', 1, 0.1, 0.01, 0.001, 0.00001]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(SVC(), param_grid, n_jobs=-1,verbose=1, scoring='f1')\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print(\"Best Parameters:\\n\", clf.best_params_)\n",
    "print(\"Best Estimators:\\n\", clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm =   SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "#predict = svm.predict(train_X)\n",
    "#CalScore(train_y,predict)\n",
    "\n",
    "predict2 = svm.predict(test_X)\n",
    "CalScore(test_y,predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1을 score로 했음에도 불구하고 성능 향상이 되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueArray = [];\n",
    "\n",
    "for i in range(28):\n",
    "    valueArray.append(data['V'+str(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "1 9\n",
      "1 10\n",
      "1 11\n",
      "1 12\n",
      "1 13\n",
      "1 14\n",
      "1 15\n",
      "1 16\n",
      "1 17\n",
      "1 18\n",
      "1 19\n",
      "1 20\n",
      "1 21\n",
      "1 22\n",
      "1 23\n",
      "1 24\n",
      "1 25\n",
      "1 26\n",
      "1 27\n",
      "1 28\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "2 9\n",
      "2 10\n",
      "2 11\n",
      "2 12\n",
      "2 13\n",
      "2 14\n",
      "2 15\n",
      "2 16\n",
      "2 17\n",
      "2 18\n",
      "2 19\n",
      "2 20\n",
      "2 21\n",
      "2 22\n",
      "2 23\n",
      "2 24\n",
      "2 25\n",
      "2 26\n",
      "2 27\n",
      "2 28\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "3 7\n",
      "3 8\n",
      "3 9\n",
      "3 10\n",
      "3 11\n",
      "3 12\n",
      "3 13\n",
      "3 14\n",
      "3 15\n",
      "3 16\n",
      "3 17\n",
      "3 18\n",
      "3 19\n",
      "3 20\n",
      "3 21\n",
      "3 22\n",
      "3 23\n",
      "3 24\n",
      "3 25\n",
      "3 26\n",
      "3 27\n",
      "3 28\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 4\n",
      "4 5\n",
      "4 6\n",
      "4 7\n",
      "4 8\n",
      "4 9\n",
      "4 10\n",
      "4 11\n",
      "4 12\n",
      "4 13\n",
      "4 14\n",
      "4 15\n",
      "4 16\n",
      "4 17\n",
      "4 18\n",
      "4 19\n",
      "4 20\n",
      "4 21\n",
      "4 22\n",
      "4 23\n",
      "4 24\n",
      "4 25\n",
      "4 26\n",
      "4 27\n",
      "4 28\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 5\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "5 9\n",
      "5 10\n",
      "5 11\n",
      "5 12\n",
      "5 13\n",
      "5 14\n",
      "5 15\n",
      "5 16\n",
      "5 17\n",
      "5 18\n",
      "5 19\n",
      "5 20\n",
      "5 21\n",
      "5 22\n",
      "5 23\n",
      "5 24\n",
      "5 25\n",
      "5 26\n",
      "5 27\n",
      "5 28\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "6 6\n",
      "6 7\n",
      "6 8\n",
      "6 9\n",
      "6 10\n",
      "6 11\n",
      "6 12\n",
      "6 13\n",
      "6 14\n",
      "6 15\n",
      "6 16\n",
      "6 17\n",
      "6 18\n",
      "6 19\n",
      "6 20\n",
      "6 21\n",
      "6 22\n",
      "6 23\n",
      "6 24\n",
      "6 25\n",
      "6 26\n",
      "6 27\n",
      "6 28\n",
      "7 1\n",
      "7 2\n",
      "7 3\n",
      "7 4\n",
      "7 5\n",
      "7 6\n",
      "7 7\n",
      "7 8\n",
      "7 9\n",
      "7 10\n",
      "7 11\n",
      "7 12\n",
      "7 13\n",
      "7 14\n",
      "7 15\n",
      "7 16\n",
      "7 17\n",
      "7 18\n",
      "7 19\n",
      "7 20\n",
      "7 21\n",
      "7 22\n",
      "7 23\n",
      "7 24\n",
      "7 25\n",
      "7 26\n",
      "7 27\n",
      "7 28\n",
      "8 1\n",
      "8 2\n",
      "8 3\n",
      "8 4\n",
      "8 5\n",
      "8 6\n",
      "8 7\n",
      "8 8\n",
      "8 9\n",
      "8 10\n",
      "8 11\n",
      "8 12\n",
      "8 13\n",
      "8 14\n",
      "8 15\n",
      "8 16\n",
      "8 17\n",
      "8 18\n",
      "8 19\n",
      "8 20\n",
      "8 21\n",
      "8 22\n",
      "8 23\n",
      "8 24\n",
      "8 25\n",
      "8 26\n",
      "8 27\n",
      "8 28\n",
      "9 1\n",
      "9 2\n",
      "9 3\n",
      "9 4\n",
      "9 5\n",
      "9 6\n",
      "9 7\n",
      "9 8\n",
      "9 9\n",
      "9 10\n",
      "9 11\n",
      "9 12\n",
      "9 13\n",
      "9 14\n",
      "9 15\n",
      "9 16\n",
      "9 17\n",
      "9 18\n",
      "9 19\n",
      "9 20\n",
      "9 21\n",
      "9 22\n",
      "9 23\n",
      "9 24\n",
      "9 25\n",
      "9 26\n",
      "9 27\n",
      "9 28\n",
      "10 1\n",
      "10 2\n",
      "10 3\n",
      "10 4\n",
      "10 5\n",
      "10 6\n",
      "10 7\n",
      "10 8\n",
      "10 9\n",
      "10 10\n",
      "10 11\n",
      "10 12\n",
      "10 13\n",
      "10 14\n",
      "10 15\n",
      "10 16\n",
      "10 17\n",
      "10 18\n",
      "10 19\n",
      "10 20\n",
      "10 21\n",
      "10 22\n",
      "10 23\n",
      "10 24\n",
      "10 25\n",
      "10 26\n",
      "10 27\n",
      "10 28\n",
      "11 1\n",
      "11 2\n",
      "11 3\n",
      "11 4\n",
      "11 5\n",
      "11 6\n",
      "11 7\n",
      "11 8\n",
      "11 9\n",
      "11 10\n",
      "11 11\n",
      "11 12\n",
      "11 13\n",
      "11 14\n",
      "11 15\n",
      "11 16\n",
      "11 17\n",
      "11 18\n",
      "11 19\n",
      "11 20\n",
      "11 21\n",
      "11 22\n",
      "11 23\n",
      "11 24\n",
      "11 25\n",
      "11 26\n",
      "11 27\n",
      "11 28\n",
      "12 1\n",
      "12 2\n",
      "12 3\n",
      "12 4\n",
      "12 5\n",
      "12 6\n",
      "12 7\n",
      "12 8\n",
      "12 9\n",
      "12 10\n",
      "12 11\n",
      "12 12\n",
      "12 13\n",
      "12 14\n",
      "12 15\n",
      "12 16\n",
      "12 17\n",
      "12 18\n",
      "12 19\n",
      "12 20\n",
      "12 21\n",
      "12 22\n",
      "12 23\n",
      "12 24\n",
      "12 25\n",
      "12 26\n",
      "12 27\n",
      "12 28\n",
      "13 1\n",
      "13 2\n",
      "13 3\n",
      "13 4\n",
      "13 5\n",
      "13 6\n",
      "13 7\n",
      "13 8\n",
      "13 9\n",
      "13 10\n",
      "13 11\n",
      "13 12\n",
      "13 13\n",
      "13 14\n",
      "13 15\n",
      "13 16\n",
      "13 17\n",
      "13 18\n",
      "13 19\n",
      "13 20\n",
      "13 21\n",
      "13 22\n",
      "13 23\n",
      "13 24\n",
      "13 25\n",
      "13 26\n",
      "13 27\n",
      "13 28\n",
      "14 1\n",
      "14 2\n",
      "14 3\n",
      "14 4\n",
      "14 5\n",
      "14 6\n",
      "14 7\n",
      "14 8\n",
      "14 9\n",
      "14 10\n",
      "14 11\n",
      "14 12\n",
      "14 13\n",
      "14 14\n",
      "14 15\n",
      "14 16\n",
      "14 17\n",
      "14 18\n",
      "14 19\n",
      "14 20\n",
      "14 21\n",
      "14 22\n",
      "14 23\n",
      "14 24\n",
      "14 25\n",
      "14 26\n",
      "14 27\n",
      "14 28\n",
      "15 1\n",
      "15 2\n",
      "15 3\n",
      "15 4\n",
      "15 5\n",
      "15 6\n",
      "15 7\n",
      "15 8\n",
      "15 9\n",
      "15 10\n",
      "15 11\n",
      "15 12\n",
      "15 13\n",
      "15 14\n",
      "15 15\n",
      "15 16\n",
      "15 17\n",
      "15 18\n",
      "15 19\n",
      "15 20\n",
      "15 21\n",
      "15 22\n",
      "15 23\n",
      "15 24\n",
      "15 25\n",
      "15 26\n",
      "15 27\n",
      "15 28\n",
      "16 1\n",
      "16 2\n",
      "16 3\n",
      "16 4\n",
      "16 5\n",
      "16 6\n",
      "16 7\n",
      "16 8\n",
      "16 9\n",
      "16 10\n",
      "16 11\n",
      "16 12\n",
      "16 13\n",
      "16 14\n",
      "16 15\n",
      "16 16\n",
      "16 17\n",
      "16 18\n",
      "16 19\n",
      "16 20\n",
      "16 21\n",
      "16 22\n",
      "16 23\n",
      "16 24\n",
      "16 25\n",
      "16 26\n",
      "16 27\n",
      "16 28\n",
      "17 1\n",
      "17 2\n",
      "17 3\n",
      "17 4\n",
      "17 5\n",
      "17 6\n",
      "17 7\n",
      "17 8\n",
      "17 9\n",
      "17 10\n",
      "17 11\n",
      "17 12\n",
      "17 13\n",
      "17 14\n",
      "17 15\n",
      "17 16\n",
      "17 17\n",
      "17 18\n",
      "17 19\n",
      "17 20\n",
      "17 21\n",
      "17 22\n",
      "17 23\n",
      "17 24\n",
      "17 25\n",
      "17 26\n",
      "17 27\n",
      "17 28\n",
      "18 1\n",
      "18 2\n",
      "18 3\n",
      "18 4\n",
      "18 5\n",
      "18 6\n",
      "18 7\n",
      "18 8\n",
      "18 9\n",
      "18 10\n",
      "18 11\n",
      "18 12\n",
      "18 13\n",
      "18 14\n",
      "18 15\n",
      "18 16\n",
      "18 17\n",
      "18 18\n",
      "18 19\n",
      "18 20\n",
      "18 21\n",
      "18 22\n",
      "18 23\n",
      "18 24\n",
      "18 25\n",
      "18 26\n",
      "18 27\n",
      "18 28\n",
      "19 1\n",
      "19 2\n",
      "19 3\n",
      "19 4\n",
      "19 5\n",
      "19 6\n",
      "19 7\n",
      "19 8\n",
      "19 9\n",
      "19 10\n",
      "19 11\n",
      "19 12\n",
      "19 13\n",
      "19 14\n",
      "19 15\n",
      "19 16\n",
      "19 17\n",
      "19 18\n",
      "19 19\n",
      "19 20\n",
      "19 21\n",
      "19 22\n",
      "19 23\n",
      "19 24\n",
      "19 25\n",
      "19 26\n",
      "19 27\n",
      "19 28\n",
      "20 1\n",
      "20 2\n",
      "20 3\n",
      "20 4\n",
      "20 5\n",
      "20 6\n",
      "20 7\n",
      "20 8\n",
      "20 9\n",
      "20 10\n",
      "20 11\n",
      "20 12\n",
      "20 13\n",
      "20 14\n",
      "20 15\n",
      "20 16\n",
      "20 17\n",
      "20 18\n",
      "20 19\n",
      "20 20\n",
      "20 21\n",
      "20 22\n",
      "20 23\n",
      "20 24\n",
      "20 25\n",
      "20 26\n",
      "20 27\n",
      "20 28\n",
      "21 1\n",
      "21 2\n",
      "21 3\n",
      "21 4\n",
      "21 5\n",
      "21 6\n",
      "21 7\n",
      "21 8\n",
      "21 9\n",
      "21 10\n",
      "21 11\n",
      "21 12\n",
      "21 13\n",
      "21 14\n",
      "21 15\n",
      "21 16\n",
      "21 17\n",
      "21 18\n",
      "21 19\n",
      "21 20\n",
      "21 21\n",
      "21 22\n",
      "21 23\n",
      "21 24\n",
      "21 25\n",
      "21 26\n",
      "21 27\n",
      "21 28\n",
      "22 1\n",
      "22 2\n",
      "22 3\n",
      "22 4\n",
      "22 5\n",
      "22 6\n",
      "22 7\n",
      "22 8\n",
      "22 9\n",
      "22 10\n",
      "22 11\n",
      "22 12\n",
      "22 13\n",
      "22 14\n",
      "22 15\n",
      "22 16\n",
      "22 17\n",
      "22 18\n",
      "22 19\n",
      "22 20\n",
      "22 21\n",
      "22 22\n",
      "22 23\n",
      "22 24\n",
      "22 25\n",
      "22 26\n",
      "22 27\n",
      "22 28\n",
      "23 1\n",
      "23 2\n",
      "23 3\n",
      "23 4\n",
      "23 5\n",
      "23 6\n",
      "23 7\n",
      "23 8\n",
      "23 9\n",
      "23 10\n",
      "23 11\n",
      "23 12\n",
      "23 13\n",
      "23 14\n",
      "23 15\n",
      "23 16\n",
      "23 17\n",
      "23 18\n",
      "23 19\n",
      "23 20\n",
      "23 21\n",
      "23 22\n",
      "23 23\n",
      "23 24\n",
      "23 25\n",
      "23 26\n",
      "23 27\n",
      "23 28\n",
      "24 1\n",
      "24 2\n",
      "24 3\n",
      "24 4\n",
      "24 5\n",
      "24 6\n",
      "24 7\n",
      "24 8\n",
      "24 9\n",
      "24 10\n",
      "24 11\n",
      "24 12\n",
      "24 13\n",
      "24 14\n",
      "24 15\n",
      "24 16\n",
      "24 17\n",
      "24 18\n",
      "24 19\n",
      "24 20\n",
      "24 21\n",
      "24 22\n",
      "24 23\n",
      "24 24\n",
      "24 25\n",
      "24 26\n",
      "24 27\n",
      "24 28\n",
      "25 1\n",
      "25 2\n",
      "25 3\n",
      "25 4\n",
      "25 5\n",
      "25 6\n",
      "25 7\n",
      "25 8\n",
      "25 9\n",
      "25 10\n",
      "25 11\n",
      "25 12\n",
      "25 13\n",
      "25 14\n",
      "25 15\n",
      "25 16\n",
      "25 17\n",
      "25 18\n",
      "25 19\n",
      "25 20\n",
      "25 21\n",
      "25 22\n",
      "25 23\n",
      "25 24\n",
      "25 25\n",
      "25 26\n",
      "25 27\n",
      "25 28\n",
      "26 1\n",
      "26 2\n",
      "26 3\n",
      "26 4\n",
      "26 5\n",
      "26 6\n",
      "26 7\n",
      "26 8\n",
      "26 9\n",
      "26 10\n",
      "26 11\n",
      "26 12\n",
      "26 13\n",
      "26 14\n",
      "26 15\n",
      "26 16\n",
      "26 17\n",
      "26 18\n",
      "26 19\n",
      "26 20\n",
      "26 21\n",
      "26 22\n",
      "26 23\n",
      "26 24\n",
      "26 25\n",
      "26 26\n",
      "26 27\n",
      "26 28\n",
      "27 1\n",
      "27 2\n",
      "27 3\n",
      "27 4\n",
      "27 5\n",
      "27 6\n",
      "27 7\n",
      "27 8\n",
      "27 9\n",
      "27 10\n",
      "27 11\n",
      "27 12\n",
      "27 13\n",
      "27 14\n",
      "27 15\n",
      "27 16\n",
      "27 17\n",
      "27 18\n",
      "27 19\n",
      "27 20\n",
      "27 21\n",
      "27 22\n",
      "27 23\n",
      "27 24\n",
      "27 25\n",
      "27 26\n",
      "27 27\n",
      "27 28\n",
      "28 1\n",
      "28 2\n",
      "28 3\n",
      "28 4\n",
      "28 5\n",
      "28 6\n",
      "28 7\n",
      "28 8\n",
      "28 9\n",
      "28 10\n",
      "28 11\n",
      "28 12\n",
      "28 13\n",
      "28 14\n",
      "28 15\n",
      "28 16\n",
      "28 17\n",
      "28 18\n",
      "28 19\n",
      "28 20\n",
      "28 21\n",
      "28 22\n",
      "28 23\n",
      "28 24\n",
      "28 25\n",
      "28 26\n",
      "28 27\n",
      "28 28\n"
     ]
    }
   ],
   "source": [
    "col = np.where(result==0,'b','r')\n",
    "\n",
    "for i in range(len(valueArray)):\n",
    "    for j in range(len(valueArray)):\n",
    "        print(i+1,j+1)\n",
    "        #plt.scatter(valueArray[i],valueArray[j],c=col,s=10)\n",
    "        #plt.savefig(\"cov\"+str(i)+str(j)+\".png\")\n",
    "        #plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-0.003370153263307601\n",
      "3.849591828427786\n",
      "1.9620376725302158\n",
      "1\n",
      "0.0009989272770197716\n",
      "2.757126388159257\n",
      "1.6604596918200867\n",
      "2\n",
      "0.0004080125078719491\n",
      "2.3041277610233655\n",
      "1.5179353612797106\n",
      "3\n",
      "0.0031398879531128274\n",
      "2.010639537183285\n",
      "1.417970217311804\n",
      "4\n",
      "0.0006059713480719164\n",
      "1.9214211708025348\n",
      "1.3861533720344712\n",
      "5\n",
      "-0.00016432192844623237\n",
      "1.7835095724498509\n",
      "1.335481026615448\n",
      "6\n",
      "0.001902354583129389\n",
      "1.5576292337595443\n",
      "1.2480501727733322\n",
      "7\n",
      "-0.0011247242026774546\n",
      "1.4312386917702324\n",
      "1.1963438852479802\n",
      "8\n",
      "0.0007226963880086407\n",
      "1.2094352889259228\n",
      "1.0997432831920015\n",
      "9\n",
      "-0.001475384838393684\n",
      "1.1971786561847864\n",
      "1.0941565958238273\n",
      "10\n",
      "0.0013898046105778248\n",
      "1.0444270844230639\n",
      "1.0219721544264617\n",
      "11\n",
      "-0.0016884974514489828\n",
      "1.0072842222987084\n",
      "1.0036355027093793\n",
      "12\n",
      "-0.0031960345874183127\n",
      "0.9910193103362939\n",
      "0.9954995280442346\n",
      "13\n",
      "-0.0009140242950301451\n",
      "0.9249473483552314\n",
      "0.9617418304073247\n",
      "14\n",
      "-0.00028474145381528735\n",
      "0.8404485267287229\n",
      "0.9167597977271489\n",
      "15\n",
      "0.000534280165461583\n",
      "0.7698000485081136\n",
      "0.8773824984054067\n",
      "16\n",
      "0.00033977601740886956\n",
      "0.7317197874281628\n",
      "0.855406211941533\n",
      "17\n",
      "0.00012167883810507705\n",
      "0.7034847238654632\n",
      "0.8387399620057836\n",
      "18\n",
      "-0.00033433001387847495\n",
      "0.6625966524555088\n",
      "0.8140004007710984\n",
      "19\n",
      "0.0005313277321418713\n",
      "0.6089856023036262\n",
      "0.780375295805567\n",
      "20\n",
      "0.0004061424504522742\n",
      "0.5443419939634345\n",
      "0.7377953604919419\n",
      "21\n",
      "-9.147479817932553e-05\n",
      "0.526941324774425\n",
      "0.7259072425416522\n",
      "22\n",
      "0.0017314962385340659\n",
      "0.37690398887545273\n",
      "0.6139250678018066\n",
      "23\n",
      "-0.0002852560794700097\n",
      "0.3663645353016506\n",
      "0.6052805426425424\n",
      "24\n",
      "0.0011505447759221038\n",
      "0.27235498223914173\n",
      "0.5218764051374059\n",
      "25\n",
      "-8.180041486965742e-05\n",
      "0.23343030298694864\n",
      "0.483146254240834\n",
      "26\n",
      "0.0007481285683442808\n",
      "0.16465237082088835\n",
      "0.40577379267381025\n",
      "27\n",
      "-6.519766352511097e-05\n",
      "0.10948384830308025\n",
      "0.3308834361268032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(valueArray)):\n",
    "    print(i)\n",
    "    print(np.mean(valueArray[i]))\n",
    "    print(np.var(valueArray[i]))\n",
    "    print(np.std(valueArray[i]))\n",
    "    #plt.scatter(time,valueArray[i],c=col,s=10)\n",
    "    #plt.savefig(\"value\"+str(i)+\".png\")\n",
    "    #plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 알고리즘의 한계를 느끼고 Nerural Network를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 20s 123us/sample - loss: 0.0207 - acc: 0.9984\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 19s 119us/sample - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - 19s 119us/sample - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 20s 122us/sample - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 19s 122us/sample - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 20s 123us/sample - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 19s 122us/sample - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 19s 120us/sample - loss: 0.0036 - acc: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b10bc15f98>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Initialising the ANN\n",
    "classifier = keras.Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(layers.Dense(units =15 , kernel_initializer = 'uniform', activation = 'relu', input_dim = 5))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(layers.Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(layers.Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(train_X, train_y, batch_size = 32, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ...\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n",
      "39873/39873 [==============================] - 2s 44us/sample - loss: 0.0042 - acc: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004205411993305984, 0.9992476]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(test_X)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(y_pred)\n",
    "score = classifier.evaluate(test_X, test_y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.83      0.76      0.79        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.91      0.88      0.90     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's see how our model performed\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "#feature 전체\n",
    "feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0130 - acc: 0.9978\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 19s 120us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - 19s 120us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 19s 122us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 19s 122us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 19s 119us/sample - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 19s 118us/sample - loss: 0.0020 - acc: 0.9995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.88      0.76      0.82        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.94      0.88      0.91     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.8169014084507042\n",
      "0.7953586497890296\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(layers.Dense(units=32, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = 28))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=10,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0120 - acc: 0.9968\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0018 - acc: 0.9995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.96      0.67      0.79        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.98      0.84      0.90     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.7906976744186047\n",
      "0.7399553571428571\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=32, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = 28))\n",
    "\n",
    "model.add(layers.Dense(units=32,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=10,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 26s 161us/sample - loss: 0.0073 - acc: 0.9990\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 26s 162us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 25s 160us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0019 - acc: 0.9995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.86      0.82      0.84        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.93      0.91      0.92     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.8378378378378377\n",
      "0.8292181069958847\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=32, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = 28))\n",
    "\n",
    "model.add(layers.Dense(units=32,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=32,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=10,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0075 - acc: 0.9991\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 28s 179us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 28s 176us/sample - loss: 0.0020 - acc: 0.9994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.79      0.80      0.80        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.90      0.90      0.90     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.7973856209150327\n",
      "0.7993951612903227\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=32, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = 28))\n",
    "\n",
    "model.add(layers.Dense(units=32,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=32,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=32,\n",
    "                        kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                        activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=10,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layer 3개일 때 최적의 결과가 나옴을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layer의 유닛과 feature를 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ml_model_select.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 26s 164us/sample - loss: 0.0163 - acc: 0.9969\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 26s 162us/sample - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - 26s 163us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 26s 164us/sample - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 26s 161us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 26s 163us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 26s 161us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 26s 163us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 27s 166us/sample - loss: 0.0026 - acc: 0.9994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.84      0.80      0.82        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.92      0.90      0.91     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.8187919463087249\n",
      "0.8125000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 14\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=hidden_unit, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = feature_count))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=10,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0097 - acc: 0.9979\n",
      "Epoch 2/10\n",
      "159491/159491 [==============================] - 28s 173us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 3/10\n",
      "159491/159491 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.999 - 28s 176us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 4/10\n",
      "159491/159491 [==============================] - 28s 173us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/10\n",
      "159491/159491 [==============================] - 27s 172us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "159491/159491 [==============================] - 28s 175us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "159491/159491 [==============================] - 28s 176us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 8/10\n",
      "159491/159491 [==============================] - 28s 177us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/10\n",
      "159491/159491 [==============================] - 28s 173us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/10\n",
      "159491/159491 [==============================] - 28s 177us/sample - loss: 0.0018 - acc: 0.9995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.93      0.75      0.83        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.97      0.87      0.92     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.832116788321168\n",
      "0.7984913793103448\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_total = feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 28\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=hidden_unit, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = feature_count))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=10,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유닛이 28개일때, feature14, feature total에서 높은 수치가 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch수를 늘려서 모델의 정확성을 높힘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ml_epoch.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 100 for feature14, feature total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0114 - acc: 0.9987\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 28s 177us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 29s 179us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 28s 176us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 28s 176us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 29s 179us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 26s 166us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 25s 159us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 25s 156us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 25s 157us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 25s 156us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 25s 155us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 26s 163us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 25s 157us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 25s 160us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 25s 157us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 25s 156us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 25s 156us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 25s 157us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 25s 157us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 25s 158us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 25s 157us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 28s 173us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 27s 168us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 28s 176us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 28s 177us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 29s 179us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 27s 168us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 27s 172us/sample - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 27s 170us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 28s 178us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 9.6977e-04 - acc: 0.9996\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 9.4769e-04 - acc: 0.9996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.82      0.80      0.81        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.91      0.90      0.91     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.8133333333333335\n",
      "0.8091836734693877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 14\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=hidden_unit, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = feature_count))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=100,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0075 - acc: 0.9987\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 9.8835e-04 - acc: 0.9997\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 9.9255e-04 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 9.4488e-04 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 9.7978e-04 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 8.8682e-04 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 8.0602e-04 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 8.8520e-04 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 7.3364e-04 - acc: 0.9998\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 7.5120e-04 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 8.6949e-04 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 7.9270e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 8.0834e-04 - acc: 0.9998\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 6.5938e-04 - acc: 0.9998\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 6.7550e-04 - acc: 0.9998\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 8.0284e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 8.5469e-04 - acc: 0.9998\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 6.6280e-04 - acc: 0.9998\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 7.4855e-04 - acc: 0.9998\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 7.3670e-04 - acc: 0.9998\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 6.5104e-04 - acc: 0.9998\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 7.0937e-04 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.8815e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 5.8082e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 7.4726e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.8864e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 7.7424e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 7.1490e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.9793e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 8.0224e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.2585e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 6.0093e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 6.7847e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 6.2295e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 6.4684e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 5.8024e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 4.1979e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 5.3858e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 7.4015e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 6.1375e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 5.8863e-04 - acc: 0.9999\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 6.0099e-04 - acc: 0.9999\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 6.5988e-04 - acc: 0.9999\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 7.4215e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 5.3738e-04 - acc: 0.9999\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 6.2929e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 4.2984e-04 - acc: 0.9999\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 4.6588e-04 - acc: 0.9999\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 4.7180e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 5.4289e-04 - acc: 0.9998\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 5.0923e-04 - acc: 0.9999\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 5.1518e-04 - acc: 0.9999\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 29s 184us/sample - loss: 5.3802e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 4.9579e-04 - acc: 0.9999\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 29s 179us/sample - loss: 5.0656e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 5.0795e-04 - acc: 0.9999\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.2621e-04 - acc: 0.9999\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 4.1094e-04 - acc: 0.9999\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 3.8030e-04 - acc: 0.9999\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 6.1484e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 3.2855e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 5.3781e-04 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 5.0460e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 4.1210e-04 - acc: 0.9999\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 4.3964e-04 - acc: 0.9999\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 3.8951e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 5.8256e-04 - acc: 0.9999\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 5.0925e-04 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 2.5792e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 3.8196e-04 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 3.4609e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.0390e-04 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 5.2965e-04 - acc: 0.9998\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 3.7398e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 3.8057e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 3.4895e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 3.1656e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 4.7118e-04 - acc: 0.9999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.87      0.76      0.81        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.93      0.88      0.91     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.8111888111888113\n",
      "0.7920168067226891\n"
     ]
    }
   ],
   "source": [
    "feature_total = feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 28\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=hidden_unit, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = feature_count))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=100,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 200 for feature14, feature total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0126 - acc: 0.9984\n",
      "Epoch 2/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 3/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 4/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 7/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 9/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 10/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 11/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 12/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 13/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 14/200\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 15/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 16/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 17/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 18/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 19/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 20/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 21/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 22/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 23/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 24/200\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 25/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 26/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 27/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 28/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 29/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 30/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 31/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 32/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 33/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 34/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 35/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 36/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 37/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 38/200\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 39/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 40/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 41/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 42/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 43/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 44/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 45/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 46/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 47/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 48/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 49/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 50/200\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 51/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 52/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 53/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 54/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 55/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 56/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 57/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 58/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 59/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 60/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 61/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 62/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 63/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 64/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 65/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 66/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 67/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 68/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 69/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 70/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9995\n",
      "Epoch 71/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 72/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 73/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 74/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 75/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 77/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 78/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 79/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 80/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 81/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 82/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 83/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 84/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 85/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 86/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 87/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 88/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 89/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 90/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 91/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 92/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.5575e-04 - acc: 0.9997\n",
      "Epoch 93/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 94/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 95/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.2095e-04 - acc: 0.9997\n",
      "Epoch 96/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 97/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 9.1091e-04 - acc: 0.9997\n",
      "Epoch 98/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 99/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 9.3400e-04 - acc: 0.9997\n",
      "Epoch 100/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.7018e-04 - acc: 0.9997\n",
      "Epoch 101/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.7876e-04 - acc: 0.9997\n",
      "Epoch 102/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 103/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 104/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 105/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 9.2458e-04 - acc: 0.9997\n",
      "Epoch 106/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 107/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.8113e-04 - acc: 0.9997\n",
      "Epoch 108/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.5226e-04 - acc: 0.9997\n",
      "Epoch 109/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 8.2770e-04 - acc: 0.9997\n",
      "Epoch 110/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 111/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 112/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 113/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 9.2404e-04 - acc: 0.9997\n",
      "Epoch 114/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 9.2882e-04 - acc: 0.9997\n",
      "Epoch 115/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 9.3131e-04 - acc: 0.9997\n",
      "Epoch 116/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 9.8826e-04 - acc: 0.9997\n",
      "Epoch 117/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 8.5681e-04 - acc: 0.9998\n",
      "Epoch 118/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 9.4637e-04 - acc: 0.9997\n",
      "Epoch 119/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 8.9959e-04 - acc: 0.9997\n",
      "Epoch 120/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 8.6444e-04 - acc: 0.9997\n",
      "Epoch 121/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.6181e-04 - acc: 0.9997\n",
      "Epoch 122/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.6524e-04 - acc: 0.9997\n",
      "Epoch 123/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 9.0126e-04 - acc: 0.9997\n",
      "Epoch 124/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 8.7524e-04 - acc: 0.9997\n",
      "Epoch 125/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 9.3439e-04 - acc: 0.9997\n",
      "Epoch 126/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 8.8481e-04 - acc: 0.9997\n",
      "Epoch 127/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.5567e-04 - acc: 0.9997\n",
      "Epoch 128/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 8.1461e-04 - acc: 0.9998\n",
      "Epoch 129/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 8.6834e-04 - acc: 0.9997\n",
      "Epoch 130/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 131/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 132/200\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.1482e-04 - acc: 0.9998\n",
      "Epoch 133/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 134/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 8.8417e-04 - acc: 0.9997\n",
      "Epoch 135/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.2799e-04 - acc: 0.9997\n",
      "Epoch 136/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.1444e-04 - acc: 0.9997\n",
      "Epoch 137/200\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.5702e-04 - acc: 0.9997\n",
      "Epoch 138/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.6015e-04 - acc: 0.9997\n",
      "Epoch 139/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.5387e-04 - acc: 0.9997\n",
      "Epoch 140/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.2180e-04 - acc: 0.9997\n",
      "Epoch 141/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.7436e-04 - acc: 0.9997\n",
      "Epoch 142/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.0264e-04 - acc: 0.9997\n",
      "Epoch 143/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.6884e-04 - acc: 0.9997\n",
      "Epoch 144/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 145/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 8.4798e-04 - acc: 0.9997\n",
      "Epoch 146/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 147/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.3734e-04 - acc: 0.9997\n",
      "Epoch 148/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.4293e-04 - acc: 0.9997\n",
      "Epoch 149/200\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 9.6038e-04 - acc: 0.9997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.9494e-04 - acc: 0.9997\n",
      "Epoch 151/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.7329e-04 - acc: 0.9998\n",
      "Epoch 152/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.4596e-04 - acc: 0.9998\n",
      "Epoch 153/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.2535e-04 - acc: 0.9997\n",
      "Epoch 154/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.8176e-04 - acc: 0.9998\n",
      "Epoch 155/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.3652e-04 - acc: 0.9997\n",
      "Epoch 156/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 157/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 158/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 6.8383e-04 - acc: 0.9998\n",
      "Epoch 159/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.7413e-04 - acc: 0.9997\n",
      "Epoch 160/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.0892e-04 - acc: 0.9998\n",
      "Epoch 161/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 8.5365e-04 - acc: 0.9997\n",
      "Epoch 162/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.7209e-04 - acc: 0.9997\n",
      "Epoch 163/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 8.7448e-04 - acc: 0.9997\n",
      "Epoch 164/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 165/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.9883e-04 - acc: 0.9997\n",
      "Epoch 166/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 8.3362e-04 - acc: 0.9997\n",
      "Epoch 167/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.9767e-04 - acc: 0.9998\n",
      "Epoch 168/200\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 169/200\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.5777e-04 - acc: 0.9998\n",
      "Epoch 170/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.1074e-04 - acc: 0.9998\n",
      "Epoch 171/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.4191e-04 - acc: 0.9997\n",
      "Epoch 172/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.2280e-04 - acc: 0.9998\n",
      "Epoch 173/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 174/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.9927e-04 - acc: 0.9997\n",
      "Epoch 175/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 9.5514e-04 - acc: 0.9997\n",
      "Epoch 176/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.5054e-04 - acc: 0.9997\n",
      "Epoch 177/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.2929e-04 - acc: 0.9998\n",
      "Epoch 178/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 7.5059e-04 - acc: 0.9998\n",
      "Epoch 179/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.8781e-04 - acc: 0.9997\n",
      "Epoch 180/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.4241e-04 - acc: 0.9997\n",
      "Epoch 181/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 8.5355e-04 - acc: 0.9997\n",
      "Epoch 182/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.3425e-04 - acc: 0.9998\n",
      "Epoch 183/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.5011e-04 - acc: 0.9997\n",
      "Epoch 184/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.5686e-04 - acc: 0.9997\n",
      "Epoch 185/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.5508e-04 - acc: 0.9998\n",
      "Epoch 186/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 9.6942e-04 - acc: 0.9998\n",
      "Epoch 187/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.5968e-04 - acc: 0.9998\n",
      "Epoch 188/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.7369e-04 - acc: 0.9997\n",
      "Epoch 189/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.1571e-04 - acc: 0.9998\n",
      "Epoch 190/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 7.7565e-04 - acc: 0.9998\n",
      "Epoch 191/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 8.0539e-04 - acc: 0.9997\n",
      "Epoch 192/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.5072e-04 - acc: 0.9998\n",
      "Epoch 193/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 7.0882e-04 - acc: 0.9997\n",
      "Epoch 194/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 195/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.9862e-04 - acc: 0.9998\n",
      "Epoch 196/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 197/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.1699e-04 - acc: 0.9997\n",
      "Epoch 198/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.1463e-04 - acc: 0.9998\n",
      "Epoch 199/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 200/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.5454e-04 - acc: 0.9998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.86      0.80      0.83        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.93      0.90      0.91     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.8299319727891157\n",
      "0.8192148760330579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 14\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=hidden_unit, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = feature_count))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=200,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0105 - acc: 0.9980\n",
      "Epoch 2/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0033 - acc: 0.9993\n",
      "Epoch 3/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/200\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 6/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 7/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 8/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 9/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 11/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 12/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 13/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 14/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 15/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 16/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 17/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 18/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 19/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 20/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 21/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 22/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 23/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 24/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 25/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 26/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 27/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 28/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.9132e-04 - acc: 0.9997\n",
      "Epoch 29/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 30/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 31/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.5569e-04 - acc: 0.9997\n",
      "Epoch 32/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 8.4547e-04 - acc: 0.9997\n",
      "Epoch 33/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.4211e-04 - acc: 0.9997\n",
      "Epoch 34/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.9051e-04 - acc: 0.9997\n",
      "Epoch 35/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.2126e-04 - acc: 0.9997\n",
      "Epoch 36/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.2451e-04 - acc: 0.9998\n",
      "Epoch 37/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 8.4220e-04 - acc: 0.9997\n",
      "Epoch 38/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.1377e-04 - acc: 0.9998\n",
      "Epoch 39/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.5559e-04 - acc: 0.9998\n",
      "Epoch 40/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.0208e-04 - acc: 0.9997\n",
      "Epoch 41/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 9.7186e-04 - acc: 0.9998\n",
      "Epoch 42/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 6.5234e-04 - acc: 0.9998\n",
      "Epoch 43/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 8.9017e-04 - acc: 0.9998\n",
      "Epoch 44/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.1026e-04 - acc: 0.9998\n",
      "Epoch 45/200\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.4512e-04 - acc: 0.9998\n",
      "Epoch 46/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.5092e-04 - acc: 0.9998\n",
      "Epoch 47/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.3914e-04 - acc: 0.9998\n",
      "Epoch 48/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 9.1882e-04 - acc: 0.9997\n",
      "Epoch 49/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 7.3206e-04 - acc: 0.9998\n",
      "Epoch 50/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.9400e-04 - acc: 0.9998\n",
      "Epoch 51/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.8705e-04 - acc: 0.9998\n",
      "Epoch 52/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.4605e-04 - acc: 0.9998\n",
      "Epoch 53/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.8668e-04 - acc: 0.9998\n",
      "Epoch 54/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 55/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.7118e-04 - acc: 0.9998\n",
      "Epoch 56/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.5990e-04 - acc: 0.9998\n",
      "Epoch 57/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.3656e-04 - acc: 0.9998\n",
      "Epoch 58/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 8.4993e-04 - acc: 0.9998\n",
      "Epoch 59/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 60/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 8.3840e-04 - acc: 0.9998\n",
      "Epoch 61/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.6594e-04 - acc: 0.9998\n",
      "Epoch 62/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.0734e-04 - acc: 0.9998\n",
      "Epoch 63/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.5828e-04 - acc: 0.9998\n",
      "Epoch 64/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.4508e-04 - acc: 0.9998\n",
      "Epoch 65/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.9127e-04 - acc: 0.9998\n",
      "Epoch 66/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.9621e-04 - acc: 0.9998\n",
      "Epoch 67/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.5805e-04 - acc: 0.9998\n",
      "Epoch 68/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.8762e-04 - acc: 0.9998\n",
      "Epoch 69/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 4.5721e-04 - acc: 0.9998\n",
      "Epoch 70/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.9150e-04 - acc: 0.9998\n",
      "Epoch 71/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.0139e-04 - acc: 0.9998\n",
      "Epoch 72/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 6.2574e-04 - acc: 0.9998\n",
      "Epoch 73/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 4.6844e-04 - acc: 0.9998\n",
      "Epoch 74/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 6.3086e-04 - acc: 0.9998\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 23s 144us/sample - loss: 4.4127e-04 - acc: 0.9999\n",
      "Epoch 76/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 5.7005e-04 - acc: 0.9998\n",
      "Epoch 77/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.0108e-04 - acc: 0.9998\n",
      "Epoch 78/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.5860e-04 - acc: 0.9999\n",
      "Epoch 79/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 4.6182e-04 - acc: 0.9998\n",
      "Epoch 80/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 3.4312e-04 - acc: 0.9999\n",
      "Epoch 81/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.7300e-04 - acc: 0.9998\n",
      "Epoch 82/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 6.6980e-04 - acc: 0.9998\n",
      "Epoch 83/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 3.4913e-04 - acc: 0.9999\n",
      "Epoch 84/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.3652e-04 - acc: 0.9998\n",
      "Epoch 85/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 5.5168e-04 - acc: 0.9998\n",
      "Epoch 86/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 4.6799e-04 - acc: 0.9998\n",
      "Epoch 87/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 5.9952e-04 - acc: 0.9998\n",
      "Epoch 88/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.0934e-04 - acc: 0.9998\n",
      "Epoch 89/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 4.5988e-04 - acc: 0.9998\n",
      "Epoch 90/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 4.2411e-04 - acc: 0.9999\n",
      "Epoch 91/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.7392e-04 - acc: 0.9998\n",
      "Epoch 92/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.7957e-04 - acc: 0.9999\n",
      "Epoch 93/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.5102e-04 - acc: 0.9998\n",
      "Epoch 94/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 3.3086e-04 - acc: 0.9999\n",
      "Epoch 95/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 4.6738e-04 - acc: 0.9998\n",
      "Epoch 96/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 3.7030e-04 - acc: 0.9999\n",
      "Epoch 97/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 4.6778e-04 - acc: 0.9999\n",
      "Epoch 98/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 2.5215e-04 - acc: 0.9999\n",
      "Epoch 99/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 3.5123e-04 - acc: 0.9999\n",
      "Epoch 100/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.0606e-04 - acc: 0.9999\n",
      "Epoch 101/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.0186e-04 - acc: 0.9998\n",
      "Epoch 102/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 8.3532e-04 - acc: 0.9998\n",
      "Epoch 103/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 4.5082e-04 - acc: 0.9998\n",
      "Epoch 104/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 5.0795e-04 - acc: 0.9999\n",
      "Epoch 105/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.5963e-04 - acc: 0.9999\n",
      "Epoch 106/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 3.7538e-04 - acc: 0.9999\n",
      "Epoch 107/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 6.8627e-04 - acc: 0.9998\n",
      "Epoch 108/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 2.8870e-04 - acc: 0.9999\n",
      "Epoch 109/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 6.4644e-04 - acc: 0.9998\n",
      "Epoch 110/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 2.9292e-04 - acc: 0.9999\n",
      "Epoch 111/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.5114e-04 - acc: 0.9998\n",
      "Epoch 112/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 2.6876e-04 - acc: 0.9999\n",
      "Epoch 113/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 5.9110e-04 - acc: 0.9998\n",
      "Epoch 114/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 3.1607e-04 - acc: 0.9999\n",
      "Epoch 115/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 3.9603e-04 - acc: 0.9998\n",
      "Epoch 116/200\n",
      "159491/159491 [==============================] - 23s 147us/sample - loss: 6.3218e-04 - acc: 0.9998\n",
      "Epoch 117/200\n",
      "159491/159491 [==============================] - 23s 146us/sample - loss: 7.8161e-04 - acc: 0.9998\n",
      "Epoch 118/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 4.2209e-04 - acc: 0.9999\n",
      "Epoch 119/200\n",
      "159491/159491 [==============================] - 23s 146us/sample - loss: 6.3459e-04 - acc: 0.9998\n",
      "Epoch 120/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 5.4509e-04 - acc: 0.9999\n",
      "Epoch 121/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 5.0343e-04 - acc: 0.9999\n",
      "Epoch 122/200\n",
      "159491/159491 [==============================] - 23s 146us/sample - loss: 3.3258e-04 - acc: 0.9999\n",
      "Epoch 123/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.2850e-04 - acc: 0.9999\n",
      "Epoch 124/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 3.7612e-04 - acc: 0.9998\n",
      "Epoch 125/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.7687e-04 - acc: 0.9999\n",
      "Epoch 126/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.2905e-04 - acc: 0.9998\n",
      "Epoch 127/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.1035e-04 - acc: 0.9999\n",
      "Epoch 128/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.4115e-04 - acc: 0.9998\n",
      "Epoch 129/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.2761e-04 - acc: 0.9999\n",
      "Epoch 130/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 3.9745e-04 - acc: 0.9999\n",
      "Epoch 131/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 4.3273e-04 - acc: 0.9999\n",
      "Epoch 132/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 3.6771e-04 - acc: 0.9999\n",
      "Epoch 133/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.2927e-04 - acc: 0.9998\n",
      "Epoch 134/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.2763e-04 - acc: 0.9999\n",
      "Epoch 135/200\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 3.9031e-04 - acc: 0.9999\n",
      "Epoch 136/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.8503e-04 - acc: 0.9999\n",
      "Epoch 137/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.0899e-04 - acc: 0.9998\n",
      "Epoch 138/200\n",
      "159491/159491 [==============================] - 19s 120us/sample - loss: 4.2506e-04 - acc: 0.9999\n",
      "Epoch 139/200\n",
      "159491/159491 [==============================] - 20s 125us/sample - loss: 2.3328e-04 - acc: 0.9999\n",
      "Epoch 140/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.2084e-04 - acc: 0.9999\n",
      "Epoch 141/200\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.9588e-04 - acc: 0.9999\n",
      "Epoch 142/200\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.1302e-04 - acc: 0.9999\n",
      "Epoch 143/200\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 3.4962e-04 - acc: 0.9999\n",
      "Epoch 144/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.4346e-04 - acc: 0.9999\n",
      "Epoch 145/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 3.7429e-04 - acc: 0.9999\n",
      "Epoch 146/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.2880e-04 - acc: 0.9999\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.9900e-04 - acc: 0.9999\n",
      "Epoch 148/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 6.7992e-04 - acc: 0.9999\n",
      "Epoch 149/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.7063e-04 - acc: 0.9999\n",
      "Epoch 150/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.4482e-04 - acc: 0.9999\n",
      "Epoch 151/200\n",
      "159491/159491 [==============================] - 23s 146us/sample - loss: 3.9908e-04 - acc: 0.9999\n",
      "Epoch 152/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 2.7035e-04 - acc: 0.9999\n",
      "Epoch 153/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 4.8279e-04 - acc: 0.9999\n",
      "Epoch 154/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.1485e-04 - acc: 0.9999\n",
      "Epoch 155/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 4.5684e-04 - acc: 0.9999\n",
      "Epoch 156/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.6946e-04 - acc: 0.9999\n",
      "Epoch 157/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.2815e-04 - acc: 0.9999\n",
      "Epoch 158/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 2.8853e-04 - acc: 0.9999\n",
      "Epoch 159/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 4.9602e-04 - acc: 0.9998\n",
      "Epoch 160/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.7341e-04 - acc: 0.9999\n",
      "Epoch 161/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 3.3265e-04 - acc: 0.9999\n",
      "Epoch 162/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 5.2415e-04 - acc: 0.9999\n",
      "Epoch 163/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.4633e-04 - acc: 0.9999\n",
      "Epoch 164/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 2.8596e-04 - acc: 0.9999\n",
      "Epoch 165/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 3.6679e-04 - acc: 0.9999\n",
      "Epoch 166/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 3.8168e-04 - acc: 0.9999\n",
      "Epoch 167/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.7709e-04 - acc: 0.9999\n",
      "Epoch 168/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 2.9480e-04 - acc: 0.9999\n",
      "Epoch 169/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 5.3789e-04 - acc: 0.9999\n",
      "Epoch 170/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 3.6369e-04 - acc: 0.9999\n",
      "Epoch 171/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 5.5942e-04 - acc: 0.9999\n",
      "Epoch 172/200\n",
      "159491/159491 [==============================] - 20s 124us/sample - loss: 3.0693e-04 - acc: 0.9999\n",
      "Epoch 173/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.4173e-04 - acc: 0.9999\n",
      "Epoch 174/200\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.9525e-04 - acc: 0.9999\n",
      "Epoch 175/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 3.8167e-04 - acc: 0.9999\n",
      "Epoch 176/200\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.4463e-04 - acc: 0.9999\n",
      "Epoch 177/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 3.0943e-04 - acc: 0.9999\n",
      "Epoch 178/200\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.5180e-04 - acc: 0.9999\n",
      "Epoch 179/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.6821e-04 - acc: 0.9999\n",
      "Epoch 180/200\n",
      "159491/159491 [==============================] - 23s 146us/sample - loss: 2.7009e-04 - acc: 0.9999\n",
      "Epoch 181/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 3.2038e-04 - acc: 0.9999\n",
      "Epoch 182/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 3.5968e-04 - acc: 0.9999\n",
      "Epoch 183/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 2.7408e-04 - acc: 0.9999\n",
      "Epoch 184/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 3.1473e-04 - acc: 0.9999\n",
      "Epoch 185/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 5.5508e-04 - acc: 0.9999\n",
      "Epoch 186/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 3.4756e-04 - acc: 0.9999\n",
      "Epoch 187/200\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 3.4347e-04 - acc: 0.9999\n",
      "Epoch 188/200\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 2.7559e-04 - acc: 0.9999\n",
      "Epoch 189/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 2.1152e-04 - acc: 0.9999\n",
      "Epoch 190/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 5.1119e-04 - acc: 0.9999\n",
      "Epoch 191/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 4.9622e-04 - acc: 0.9999\n",
      "Epoch 192/200\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.8972e-04 - acc: 0.9999\n",
      "Epoch 193/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 5.7750e-04 - acc: 0.9999\n",
      "Epoch 194/200\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 1.4877e-04 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 3.8964e-04 - acc: 0.9999\n",
      "Epoch 196/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 5.5439e-04 - acc: 0.9999\n",
      "Epoch 197/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 3.0976e-04 - acc: 0.9999\n",
      "Epoch 198/200\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 3.0112e-04 - acc: 0.9999\n",
      "Epoch 199/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 3.7501e-04 - acc: 0.9999\n",
      "Epoch 200/200\n",
      "159491/159491 [==============================] - 23s 144us/sample - loss: 3.7745e-04 - acc: 0.9999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39797\n",
      "           1       0.86      0.72      0.79        76\n",
      "\n",
      "    accuracy                           1.00     39873\n",
      "   macro avg       0.93      0.86      0.89     39873\n",
      "weighted avg       1.00      1.00      1.00     39873\n",
      "\n",
      "0.7857142857142857\n",
      "0.7606382978723405\n"
     ]
    }
   ],
   "source": [
    "feature_total = feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 28\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(units=hidden_unit, \n",
    "                       kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu',\n",
    "                       input_dim = feature_count))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=hidden_unit,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=1,\n",
    "                       kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                       activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_X, train_y,epochs=200,batch_size=32)\n",
    "\n",
    "\n",
    "#검증\n",
    "predict = model.predict(test_X)\n",
    "predict = (predict > 0.5)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predict))\n",
    "\n",
    "\n",
    "predict_y = predict.reshape(-1)\n",
    "CalScore(test_y,predict_y)\n",
    "CalBetaScore(test_y,predict_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Average(Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0099 - acc: 0.9975\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 29s 180us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0017 - acc: 0.9994\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 29s 182us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 29s 183us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0084 - acc: 0.9989\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 9.7577e-04 - acc: 0.9997\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 9.1068e-04 - acc: 0.9997\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 9.8962e-04 - acc: 0.9997\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 9.3778e-04 - acc: 0.9997\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 8.8325e-04 - acc: 0.9997\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 8.7009e-04 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 9.3513e-04 - acc: 0.9996\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 29s 184us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 9.5921e-04 - acc: 0.9997\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 8.3164e-04 - acc: 0.9997\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 7.8389e-04 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 9.3963e-04 - acc: 0.9997\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 6.8656e-04 - acc: 0.9997\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 7.9369e-04 - acc: 0.9997\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0105 - acc: 0.9983\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 185us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 29s 185us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0065 - acc: 0.9992\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0035 - acc: 0.9994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 30s 186us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0102 - acc: 0.9982\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - ETA: 0s - loss: 0.0013 - acc: 0.999 - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.8703e-04 - acc: 0.9997\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 9.4379e-04 - acc: 0.9997\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.4177e-04 - acc: 0.9996\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.3715e-04 - acc: 0.9997\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.8865e-04 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 9.9190e-04 - acc: 0.9997\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 8.4255e-04 - acc: 0.9997\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0091 - acc: 0.9989\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 30s 187us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 30s 188us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 9.7624e-04 - acc: 0.9997\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.9274e-04 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 9.3286e-04 - acc: 0.9997\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 9.7174e-04 - acc: 0.9996\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 9.4821e-04 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.1004e-04 - acc: 0.9997\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 9.6683e-04 - acc: 0.9997\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 9.6314e-04 - acc: 0.9997\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0109 - acc: 0.9981\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 28s 175us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 28s 175us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 29s 181us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9995\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 30s 190us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 30s 189us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0082 - acc: 0.9991\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.7273e-04 - acc: 0.9997\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 9.6159e-04 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 31s 191us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 9.8049e-04 - acc: 0.9996\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 32s 200us/sample - loss: 0.0084 - acc: 0.9989\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0019 - acc: 0.9994\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0018 - acc: 0.9994\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 30s 191us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.5822e-04 - acc: 0.9997\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 9.7256e-04 - acc: 0.9996\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.5858e-04 - acc: 0.9997\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 9.9947e-04 - acc: 0.9997\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 9.4479e-04 - acc: 0.9997\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.9171e-04 - acc: 0.9997\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 9.2346e-04 - acc: 0.9997\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.7215e-04 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 9.2535e-04 - acc: 0.9997\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 8.6782e-04 - acc: 0.9997\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.9555e-04 - acc: 0.9997\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 9.1145e-04 - acc: 0.9997\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 9.3982e-04 - acc: 0.9997\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 9.0188e-04 - acc: 0.9997\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 32s 201us/sample - loss: 0.0095 - acc: 0.9982\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 31s 192us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 32s 198us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 32s 198us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 32s 199us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 31s 197us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 31s 193us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 31s 196us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 31s 194us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 31s 195us/sample - loss: 0.0013 - acc: 0.9996\n"
     ]
    }
   ],
   "source": [
    "feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 14\n",
    "\n",
    "n_members = 10\n",
    "for i in range(n_members):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(units=hidden_unit, \n",
    "                           kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='relu',\n",
    "                           input_dim = feature_count))\n",
    "\n",
    "    model.add(layers.Dense(units=hidden_unit,\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(units=hidden_unit,\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(units=1,\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_X, train_y,epochs=100,batch_size=32)\n",
    "    \n",
    "    filename = 'model/model14x28x28x28x1_'+ str(i + 1)+ '.h5'\n",
    "    \n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8652482269503547\n",
      "0.8400423728813559\n"
     ]
    }
   ],
   "source": [
    "feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "predict_list = np.zeros(39873);\n",
    "\n",
    "n_members = 10\n",
    "for i in range(n_members):\n",
    "    model = load_model('model/model14x28x28x28x1_'+ str(i + 1)+ '.h5')\n",
    "    \n",
    "    #검증 값 저장\n",
    "    predict = model.predict(test_X)\n",
    "    predict_y = predict.reshape(-1)\n",
    "    \n",
    "    predict_list = predict_list + predict_y\n",
    "    \n",
    "predict_list = predict_list/10\n",
    "\n",
    "predict = (predict_list > 0.5)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\opp06\\Anaconda3\\envs\\MLProject\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\opp06\\Anaconda3\\envs\\MLProject\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 23s 146us/sample - loss: 0.0089 - acc: 0.9986\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 20s 125us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 20s 125us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 21s 129us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 19s 121us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 19s 119us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 20s 128us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.8530e-04 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 9.2694e-04 - acc: 0.9997\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 9.2547e-04 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 9.8863e-04 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.5473e-04 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.7404e-04 - acc: 0.9998\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 8.8171e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 9.9452e-04 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 7.9581e-04 - acc: 0.9998\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.0301e-04 - acc: 0.9998\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 9.4078e-04 - acc: 0.9997\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 8.7425e-04 - acc: 0.9997\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 6.0025e-04 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 8.4944e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 6.4182e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.8889e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 9.4913e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 6.3336e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.5625e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.1565e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 6.5271e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.9686e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 6.4212e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 6.5301e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.3605e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 6.9389e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.5177e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 4.6506e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.2320e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.0612e-04 - acc: 0.9999\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 7.8472e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.6619e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.5915e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.7353e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 6.6017e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.8233e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 6.6362e-04 - acc: 0.9998\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 21s 132us/sample - loss: 3.8945e-04 - acc: 0.9999\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.4502e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 2.9648e-04 - acc: 0.9999\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 7.6167e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.6459e-04 - acc: 0.9998\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 4.4808e-04 - acc: 0.9999\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 7.2124e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 4.8111e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 5.5591e-04 - acc: 0.9999\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 6.3300e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 20s 127us/sample - loss: 5.3110e-04 - acc: 0.9998\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 20s 128us/sample - loss: 3.9947e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 4.1808e-04 - acc: 0.9999\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 5.7476e-04 - acc: 0.9998\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.5171e-04 - acc: 0.9999\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.7937e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.4555e-04 - acc: 0.9999\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.5786e-04 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.1981e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 4.8224e-04 - acc: 0.9999\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 5.6804e-04 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 3.3757e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 5.4855e-04 - acc: 0.9998\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 3.6585e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.5714e-04 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.1535e-04 - acc: 0.9998\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 6.2719e-04 - acc: 0.9998\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 4.5657e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 4.4856e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 4.5641e-04 - acc: 0.9998\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 3.5193e-04 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0094 - acc: 0.9977\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 20s 128us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.8361e-04 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.9634e-04 - acc: 0.9996\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.8472e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.2293e-04 - acc: 0.9998\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.2213e-04 - acc: 0.9997\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.7566e-04 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.1537e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.4293e-04 - acc: 0.9998\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.3409e-04 - acc: 0.9998\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.5239e-04 - acc: 0.9997\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 8.3020e-04 - acc: 0.9997\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.8652e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 7.8647e-04 - acc: 0.9997\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.5477e-04 - acc: 0.9997\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.7682e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.0556e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.6730e-04 - acc: 0.9997\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 7.3956e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.3718e-04 - acc: 0.9997\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.2388e-04 - acc: 0.9997\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 6.4478e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 8.2528e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.6158e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 8.2462e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.7697e-04 - acc: 0.9997\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.6910e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 7.1039e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.5073e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.0350e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.9814e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.0619e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 21s 130us/sample - loss: 7.4556e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.4525e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.4984e-04 - acc: 0.9998\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.3785e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 7.0762e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.2389e-04 - acc: 0.9998\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 7.8000e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.1030e-04 - acc: 0.9999\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 7.9850e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.9078e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.1868e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.8653e-04 - acc: 0.9998\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.4566e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.6541e-04 - acc: 0.9998\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.7944e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.9455e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 5.5818e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.3248e-04 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.8879e-04 - acc: 0.9999\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 9.2451e-04 - acc: 0.9998\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.7524e-04 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 3.9933e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.8324e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 5.8123e-04 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.6498e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.5496e-04 - acc: 0.9998\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.1046e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 4.9916e-04 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 3.3119e-04 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 9.3238e-04 - acc: 0.9998\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.3336e-04 - acc: 0.9998\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.4528e-04 - acc: 0.9998\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 4.0009e-04 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0106 - acc: 0.9972\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.5115e-04 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.9764e-04 - acc: 0.9997\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.6378e-04 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.0380e-04 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.0883e-04 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.4331e-04 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.1696e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.4313e-04 - acc: 0.9997\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.9388e-04 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.9911e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.9972e-04 - acc: 0.9998\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.8095e-04 - acc: 0.9998\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.5416e-04 - acc: 0.9998\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.2638e-04 - acc: 0.9998\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.6619e-04 - acc: 0.9998\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.3690e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.8697e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.1099e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.9340e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 5.2727e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.8117e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.9725e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.0309e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.4059e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 7.9141e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.4997e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.7594e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.4051e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.8852e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.6494e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.6119e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 4.4435e-04 - acc: 0.9999\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.8135e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.2915e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.3745e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.4757e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.4247e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.5701e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.7625e-04 - acc: 0.9998\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.7895e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.0724e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.9766e-04 - acc: 0.9999\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.6952e-04 - acc: 0.9999\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.2496e-04 - acc: 0.9998\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.5982e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 5.2472e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.4959e-04 - acc: 0.9999\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.6706e-04 - acc: 0.9998\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 3.9914e-04 - acc: 0.9999\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.6107e-04 - acc: 0.9998\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.6347e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.8818e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 3.0625e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 5.0920e-04 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 4.3186e-04 - acc: 0.9999\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.7887e-04 - acc: 0.9999\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.3146e-04 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.2629e-04 - acc: 0.9998\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 4.5322e-04 - acc: 0.9999\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.4583e-04 - acc: 0.9998\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 21s 133us/sample - loss: 3.5754e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.7346e-04 - acc: 0.9998\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 4.4564e-04 - acc: 0.9998\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 3.8271e-04 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.5992e-04 - acc: 0.9998\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 5.9350e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.6452e-04 - acc: 0.9998\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 5.0727e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.0420e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.7001e-04 - acc: 0.9998\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0085 - acc: 0.9990\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 9.6034e-04 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.2489e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.1159e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.7744e-04 - acc: 0.9997\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.8574e-04 - acc: 0.9998\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.6647e-04 - acc: 0.9997\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.5855e-04 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.2757e-04 - acc: 0.9997\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.1694e-04 - acc: 0.9997\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.2047e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 8.6244e-04 - acc: 0.9997\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.9677e-04 - acc: 0.9997\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.7071e-04 - acc: 0.9997\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.4673e-04 - acc: 0.9997\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.9446e-04 - acc: 0.9997\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.0383e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.7275e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.7612e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.0960e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.1011e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.2513e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.1984e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.0133e-04 - acc: 0.9997\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.0996e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.3189e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.6580e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.9725e-04 - acc: 0.9998\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.6122e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.1403e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 5.5641e-04 - acc: 0.9997\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.4633e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 7.0225e-04 - acc: 0.9998\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.6833e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.9055e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.3831e-04 - acc: 0.9997\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 5.9623e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.8837e-04 - acc: 0.9998\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.5639e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.8575e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.5845e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.4347e-04 - acc: 0.9998\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.1345e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.3254e-04 - acc: 0.9998\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.0030e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.7285e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.9773e-04 - acc: 0.9998\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.3504e-04 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.7927e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.6228e-04 - acc: 0.9998\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.6163e-04 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.4033e-04 - acc: 0.9998\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.1540e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 5.1195e-04 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 4.7570e-04 - acc: 0.9998\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 4.5836e-04 - acc: 0.9998\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.4126e-04 - acc: 0.9998\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.2450e-04 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.5198e-04 - acc: 0.9998\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.3162e-04 - acc: 0.9998\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.1590e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.4808e-04 - acc: 0.9998\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.8921e-04 - acc: 0.9998\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 4.9363e-04 - acc: 0.9998\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0015 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.1838e-04 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.4772e-04 - acc: 0.9996\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.6298e-04 - acc: 0.9996\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.8473e-04 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.5642e-04 - acc: 0.9997\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.6135e-04 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.5501e-04 - acc: 0.9998\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.4958e-04 - acc: 0.9997\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.7714e-04 - acc: 0.9997\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.7701e-04 - acc: 0.9997\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.6303e-04 - acc: 0.9997\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.6549e-04 - acc: 0.9997\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.6910e-04 - acc: 0.9997\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.5248e-04 - acc: 0.9997\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.1627e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.8561e-04 - acc: 0.9997\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.4327e-04 - acc: 0.9997\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.9564e-04 - acc: 0.9997\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.1899e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.1239e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.4665e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.8931e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.5466e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.5321e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.2649e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.4578e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.0011e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.8387e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.0176e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.7975e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.0788e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.6322e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.5088e-04 - acc: 0.9999\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.9103e-04 - acc: 0.9999\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.5246e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.4276e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.3445e-04 - acc: 0.9998\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.3663e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.8699e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.3427e-04 - acc: 0.9999\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.0160e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.5961e-04 - acc: 0.9998\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.7739e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.5922e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.8467e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.0522e-04 - acc: 0.9998\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.3682e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.1270e-04 - acc: 0.9999\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 6.4365e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.3413e-04 - acc: 0.9999\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.2602e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.8842e-04 - acc: 0.9999\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.8180e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.4384e-04 - acc: 0.9998\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.3965e-04 - acc: 0.9999\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.5711e-04 - acc: 0.9998\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.9976e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 3.9258e-04 - acc: 0.9999\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.5233e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.2697e-04 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.8394e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.2503e-04 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.2131e-04 - acc: 0.9998\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 2.8530e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.7962e-04 - acc: 0.9998\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.9859e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.1182e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.2803e-04 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0090 - acc: 0.9989\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 9.9036e-04 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.5254e-04 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.8896e-04 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.9108e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.7305e-04 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 9.4376e-04 - acc: 0.9997\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 20s 126us/sample - loss: 9.6273e-04 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 8.4335e-04 - acc: 0.9998\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.4831e-04 - acc: 0.9997\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.6302e-04 - acc: 0.9997\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.4789e-04 - acc: 0.9998\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.6505e-04 - acc: 0.9997\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.6886e-04 - acc: 0.9997\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 7.9801e-04 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.5276e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.3530e-04 - acc: 0.9997\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.0197e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.5140e-04 - acc: 0.9997\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.0533e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.6649e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.5058e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.8146e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.6526e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 7.2369e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.1101e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.9648e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.7174e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.1834e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.4471e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.2538e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.6994e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 6.5079e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.9428e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.7172e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.5175e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.9788e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.1251e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.7796e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.6871e-04 - acc: 0.9999\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.6682e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.1739e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.8133e-04 - acc: 0.9998\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 7.4540e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 3.5556e-04 - acc: 0.9999\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.7232e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.8063e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.3695e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.6271e-04 - acc: 0.9999\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.7501e-04 - acc: 0.9999\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.5837e-04 - acc: 0.9999\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.2676e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 4.3925e-04 - acc: 0.9999\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.1644e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.6736e-04 - acc: 0.9999\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.8783e-04 - acc: 0.9999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 3.2925e-04 - acc: 0.9999\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.3314e-04 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 21s 131us/sample - loss: 3.8952e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.3149e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.8963e-04 - acc: 0.9999\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 2.9110e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.8434e-04 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.8366e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.0305e-04 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.4865e-04 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.9949e-04 - acc: 0.9998\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 4.5208e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.5306e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.7530e-04 - acc: 0.9998\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.0024e-04 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0094 - acc: 0.9985\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0023 - acc: 0.9994\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.9569e-04 - acc: 0.9997\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.0221e-04 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.8745e-04 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.8820e-04 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.6195e-04 - acc: 0.9998\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.9189e-04 - acc: 0.9998\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.8716e-04 - acc: 0.9998\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.3828e-04 - acc: 0.9998\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.4112e-04 - acc: 0.9998\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.2484e-04 - acc: 0.9997\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.7988e-04 - acc: 0.9998\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 8.2527e-04 - acc: 0.9998\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.0369e-04 - acc: 0.9998\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.3006e-04 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.4514e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.0959e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.0631e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.3323e-04 - acc: 0.9999\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.7812e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.1222e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.5035e-04 - acc: 0.9999\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.5744e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.7962e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 9.0947e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 5.9444e-04 - acc: 0.9999\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.6311e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.6227e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 21s 133us/sample - loss: 9.4032e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.9920e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 7.2725e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.3680e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.3809e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.9578e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.9748e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 4.3140e-04 - acc: 0.9999\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 21s 133us/sample - loss: 6.5533e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.1623e-04 - acc: 0.9999\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.8186e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.3284e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.5389e-04 - acc: 0.9998\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.0735e-04 - acc: 0.9999\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.8842e-04 - acc: 0.9998\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.9450e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.4997e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.7450e-04 - acc: 0.9999\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.5923e-04 - acc: 0.9999\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.3657e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.5274e-04 - acc: 0.9998\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.1876e-04 - acc: 0.9999\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.4509e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.1203e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 5.5522e-04 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.5769e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 3.4542e-04 - acc: 0.9999\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 3.7204e-04 - acc: 0.9999\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.4869e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.5363e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.3820e-04 - acc: 0.9999\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 3.6340e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.3214e-04 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 4.5158e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 5.8090e-04 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.0494e-04 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.9608e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 3.2833e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 6.7870e-04 - acc: 0.9998\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.4047e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 4.6966e-04 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0099 - acc: 0.9976\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0016 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 9.8138e-04 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.7304e-04 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.2369e-04 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.7886e-04 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.8773e-04 - acc: 0.9997\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9998\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.1805e-04 - acc: 0.9998\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.4381e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 9.5777e-04 - acc: 0.9997\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.3733e-04 - acc: 0.9998\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 8.4911e-04 - acc: 0.9997\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.8650e-04 - acc: 0.9998\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.4805e-04 - acc: 0.9998\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.3148e-04 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 8.1225e-04 - acc: 0.9997\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.8531e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.2427e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 6.9178e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.6877e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.2100e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.1206e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.2279e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.3595e-04 - acc: 0.9997\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.3266e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.8749e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 8.2861e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.1767e-04 - acc: 0.9999\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.4696e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.2453e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 5.6677e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.5911e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.9095e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.2227e-04 - acc: 0.9999\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.1967e-04 - acc: 0.9999\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.6857e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.4351e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.0426e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.7610e-04 - acc: 0.9998\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.8982e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.4181e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.8761e-04 - acc: 0.9998\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.5024e-04 - acc: 0.9999\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.3857e-04 - acc: 0.9999\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.8465e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.3325e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.7619e-04 - acc: 0.9999\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.2126e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.3874e-04 - acc: 0.9998\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.8238e-04 - acc: 0.9999\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.3187e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.8886e-04 - acc: 0.9998\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.9010e-04 - acc: 0.9999\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.0271e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.0195e-04 - acc: 0.9998\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.0457e-04 - acc: 0.9999\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.4147e-04 - acc: 0.9998\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.4588e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.8403e-04 - acc: 0.9999\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.8773e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.7750e-04 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.4588e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.3353e-04 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.5666e-04 - acc: 0.9998\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.9709e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.0353e-04 - acc: 0.9998\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 8.6688e-04 - acc: 0.9998\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.7143e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.0819e-04 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0079 - acc: 0.9987\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.4649e-04 - acc: 0.9998\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.2939e-04 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.8719e-04 - acc: 0.9997\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.3904e-04 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 9.5482e-04 - acc: 0.9998\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 21s 135us/sample - loss: 9.0856e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 19s 122us/sample - loss: 9.2214e-04 - acc: 0.9998\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 23s 145us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.0271e-04 - acc: 0.9998\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.6488e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.7375e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.8301e-04 - acc: 0.9997\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 8.6642e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.6700e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.1647e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.3122e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 9.8446e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 7.6530e-04 - acc: 0.9998\n",
      "Epoch 55/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.9389e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.3447e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.5669e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 7.5919e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 6.7903e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.7858e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.2243e-04 - acc: 0.9998\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.1214e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.4642e-04 - acc: 0.9998\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.0417e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.8015e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.1167e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 6.0143e-04 - acc: 0.9998\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.9083e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.8736e-04 - acc: 0.9998\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.3667e-04 - acc: 0.9998\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.2227e-04 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.2205e-04 - acc: 0.9999\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 7.9726e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 5.9611e-04 - acc: 0.9998\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.3409e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.2469e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.0338e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.4864e-04 - acc: 0.9999\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.6121e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.1469e-04 - acc: 0.9998\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 141us/sample - loss: 8.5042e-04 - acc: 0.9998\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 8.3742e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.5717e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.6979e-04 - acc: 0.9998\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 6.9586e-04 - acc: 0.9998\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.7395e-04 - acc: 0.9999\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.4346e-04 - acc: 0.9999\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.3311e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.5181e-04 - acc: 0.9999\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.3129e-04 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 3.5430e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.2082e-04 - acc: 0.9998\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 6.7438e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.7321e-04 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.0323e-04 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.8817e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.7151e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 4.8008e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 21s 134us/sample - loss: 3.9886e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.2454e-04 - acc: 0.9998\n",
      "Epoch 1/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0080 - acc: 0.9990\n",
      "Epoch 2/100\n",
      "159491/159491 [==============================] - 22s 135us/sample - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 3/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 4/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 5/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 7/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 8/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 9/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 10/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 11/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0018 - acc: 0.9995\n",
      "Epoch 12/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 13/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 14/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0016 - acc: 0.9996\n",
      "Epoch 15/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9995\n",
      "Epoch 16/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 17/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 18/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 19/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 20/100\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 21/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 22/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 23/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 24/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 25/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 0.0011 - acc: 0.9997\n",
      "Epoch 26/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 27/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.3544e-04 - acc: 0.9997\n",
      "Epoch 28/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0011 - acc: 0.9996\n",
      "Epoch 29/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 30/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0010 - acc: 0.9996\n",
      "Epoch 31/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 9.6306e-04 - acc: 0.9997\n",
      "Epoch 32/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 9.8768e-04 - acc: 0.9997\n",
      "Epoch 33/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 34/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 9.7424e-04 - acc: 0.9996\n",
      "Epoch 35/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 36/100\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 7.5790e-04 - acc: 0.9997\n",
      "Epoch 37/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 9.1786e-04 - acc: 0.9997\n",
      "Epoch 38/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 9.9754e-04 - acc: 0.9997\n",
      "Epoch 39/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 8.5056e-04 - acc: 0.9997\n",
      "Epoch 40/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 41/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 7.3518e-04 - acc: 0.9998\n",
      "Epoch 42/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.4707e-04 - acc: 0.9998\n",
      "Epoch 43/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.5679e-04 - acc: 0.9998\n",
      "Epoch 44/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 7.5752e-04 - acc: 0.9997\n",
      "Epoch 45/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.9923e-04 - acc: 0.9998\n",
      "Epoch 46/100\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 6.4698e-04 - acc: 0.9998\n",
      "Epoch 47/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 7.6634e-04 - acc: 0.9998\n",
      "Epoch 48/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 7.4045e-04 - acc: 0.9998\n",
      "Epoch 49/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 6.0670e-04 - acc: 0.9998\n",
      "Epoch 50/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 6.7482e-04 - acc: 0.9998\n",
      "Epoch 51/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.7993e-04 - acc: 0.9998\n",
      "Epoch 52/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 5.4751e-04 - acc: 0.9998\n",
      "Epoch 53/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 7.2461e-04 - acc: 0.9998\n",
      "Epoch 54/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.9885e-04 - acc: 0.9998\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.9650e-04 - acc: 0.9998\n",
      "Epoch 56/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.0222e-04 - acc: 0.9998\n",
      "Epoch 57/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.9876e-04 - acc: 0.9998\n",
      "Epoch 58/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.4183e-04 - acc: 0.9998\n",
      "Epoch 59/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.8864e-04 - acc: 0.9998\n",
      "Epoch 60/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 6.1731e-04 - acc: 0.9998\n",
      "Epoch 61/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.2801e-04 - acc: 0.9999\n",
      "Epoch 62/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.4749e-04 - acc: 0.9998\n",
      "Epoch 63/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.2171e-04 - acc: 0.9999\n",
      "Epoch 64/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.3119e-04 - acc: 0.9998\n",
      "Epoch 65/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 4.8261e-04 - acc: 0.9998\n",
      "Epoch 66/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 5.2717e-04 - acc: 0.9998\n",
      "Epoch 67/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.1364e-04 - acc: 0.9999\n",
      "Epoch 68/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.9718e-04 - acc: 0.9998\n",
      "Epoch 69/100\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 4.3889e-04 - acc: 0.9999\n",
      "Epoch 70/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.4497e-04 - acc: 0.9999\n",
      "Epoch 71/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.2820e-04 - acc: 0.9999\n",
      "Epoch 72/100\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 5.7655e-04 - acc: 0.9998\n",
      "Epoch 73/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.1996e-04 - acc: 0.9998\n",
      "Epoch 74/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.3949e-04 - acc: 0.9999\n",
      "Epoch 75/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.3396e-04 - acc: 0.9998\n",
      "Epoch 76/100\n",
      "159491/159491 [==============================] - 23s 141us/sample - loss: 5.0495e-04 - acc: 0.9998\n",
      "Epoch 77/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.9048e-04 - acc: 0.9998\n",
      "Epoch 78/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 3.9515e-04 - acc: 0.9998\n",
      "Epoch 79/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 3.8641e-04 - acc: 0.9998\n",
      "Epoch 80/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 4.0495e-04 - acc: 0.9999\n",
      "Epoch 81/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 2.4885e-04 - acc: 0.9999\n",
      "Epoch 82/100\n",
      "159491/159491 [==============================] - 23s 143us/sample - loss: 6.2996e-04 - acc: 0.9998\n",
      "Epoch 83/100\n",
      "159491/159491 [==============================] - 22s 136us/sample - loss: 3.5643e-04 - acc: 0.9999\n",
      "Epoch 84/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 4.0631e-04 - acc: 0.9999\n",
      "Epoch 85/100\n",
      "159491/159491 [==============================] - 22s 138us/sample - loss: 3.4732e-04 - acc: 0.9999\n",
      "Epoch 86/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 6.7565e-04 - acc: 0.9998\n",
      "Epoch 87/100\n",
      "159491/159491 [==============================] - 22s 139us/sample - loss: 4.0677e-04 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 2.8947e-04 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.2541e-04 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 5.4940e-04 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.1675e-04 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "159491/159491 [==============================] - 22s 141us/sample - loss: 5.2933e-04 - acc: 0.9999\n",
      "Epoch 93/100\n",
      "159491/159491 [==============================] - 23s 142us/sample - loss: 5.2717e-04 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "159491/159491 [==============================] - 21s 132us/sample - loss: 3.0424e-04 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "159491/159491 [==============================] - 20s 125us/sample - loss: 4.6392e-04 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 4.0057e-04 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.8754e-04 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 3.3771e-04 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "159491/159491 [==============================] - 22s 140us/sample - loss: 5.2264e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "159491/159491 [==============================] - 22s 137us/sample - loss: 3.3402e-04 - acc: 0.9999\n"
     ]
    }
   ],
   "source": [
    "feature_total = feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "hidden_unit = 28\n",
    "feature_count = 28\n",
    "\n",
    "n_members = 10\n",
    "for i in range(n_members):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(units=hidden_unit, \n",
    "                           kernel_initializer= keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='relu',\n",
    "                           input_dim = feature_count))\n",
    "\n",
    "    model.add(layers.Dense(units=hidden_unit,\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(units=hidden_unit,\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(units=1,\n",
    "                           kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                           activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_X, train_y,epochs=100,batch_size=32)\n",
    "    \n",
    "    filename = 'model/model28x28x28x28x1_'+ str(i + 1)+ '.h5'\n",
    "    \n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838235294117647\n",
      "0.801948051948052\n"
     ]
    }
   ],
   "source": [
    "feature_total = feature_total = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15',\n",
    "              'V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "predict_list = np.zeros(39873);\n",
    "\n",
    "n_members = 10\n",
    "for i in range(n_members):\n",
    "    model = load_model('model/model28x28x28x28x1_'+ str(i + 1)+ '.h5')\n",
    "    \n",
    "    #검증 값 저장\n",
    "    predict = model.predict(test_X)\n",
    "    predict_y = predict.reshape(-1)\n",
    "    \n",
    "    predict_list = predict_list + predict_y\n",
    "    \n",
    "predict_list = predict_list/10\n",
    "\n",
    "predict = (predict_list > 0.5)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 좋은 학습 값은 Neural Network 3 Hidden Layer 10 time Ensemble (with feature 14)로 나왔다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앙상블은 feature14에 대해 100epoch을 트레이닝한 model 10개의 결과를 평균내어 사용하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8652482269503547\n",
      "0.8400423728813559\n"
     ]
    }
   ],
   "source": [
    "feature_total = ['V1','V2','V3','V4','V5','V7','V9','V10','V11','V12','V14','V16','V17','V18'];\n",
    "\n",
    "train_X,train_y = SelectFeature(train_set,feature_total);\n",
    "test_X , test_y = SelectFeature(test_set,feature_total);\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "predict_list = np.zeros(39873);\n",
    "\n",
    "n_members = 10\n",
    "for i in range(n_members):\n",
    "    model = load_model('model/model14x28x28x28x1_'+ str(i + 1)+ '.h5')\n",
    "    \n",
    "    #검증 값 저장\n",
    "    predict = model.predict(test_X)\n",
    "    predict_y = predict.reshape(-1)\n",
    "    \n",
    "    predict_list = predict_list + predict_y\n",
    "    \n",
    "predict_list = predict_list/10\n",
    "\n",
    "predict = (predict_list > 0.5)\n",
    "CalScore(test_y,predict)\n",
    "CalBetaScore(test_y,predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 예측 확률 값과 예측 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.23809004e-05\n",
      " 0.00000000e+00 8.97165058e-20]\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "#확률값\n",
    "print(predict_list)\n",
    "#예측 Class\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.000000e+00\n",
      "1        0.000000e+00\n",
      "2        0.000000e+00\n",
      "3        0.000000e+00\n",
      "4        0.000000e+00\n",
      "5        0.000000e+00\n",
      "6        0.000000e+00\n",
      "7        0.000000e+00\n",
      "8        1.490116e-08\n",
      "9        0.000000e+00\n",
      "10       0.000000e+00\n",
      "11       0.000000e+00\n",
      "12       0.000000e+00\n",
      "13       2.950430e-07\n",
      "14       1.192093e-08\n",
      "15       3.676713e-05\n",
      "16       0.000000e+00\n",
      "17       4.142523e-07\n",
      "18       0.000000e+00\n",
      "19       0.000000e+00\n",
      "20       0.000000e+00\n",
      "21       5.960464e-09\n",
      "22       0.000000e+00\n",
      "23       0.000000e+00\n",
      "24       0.000000e+00\n",
      "25       0.000000e+00\n",
      "26       0.000000e+00\n",
      "27       0.000000e+00\n",
      "28       0.000000e+00\n",
      "29       0.000000e+00\n",
      "             ...     \n",
      "39843    0.000000e+00\n",
      "39844    0.000000e+00\n",
      "39845    0.000000e+00\n",
      "39846    6.854534e-08\n",
      "39847    0.000000e+00\n",
      "39848    0.000000e+00\n",
      "39849    0.000000e+00\n",
      "39850    0.000000e+00\n",
      "39851    0.000000e+00\n",
      "39852    1.519918e-07\n",
      "39853    0.000000e+00\n",
      "39854    0.000000e+00\n",
      "39855    0.000000e+00\n",
      "39856    0.000000e+00\n",
      "39857    0.000000e+00\n",
      "39858    0.000000e+00\n",
      "39859    5.394220e-06\n",
      "39860    1.766980e-05\n",
      "39861    0.000000e+00\n",
      "39862    0.000000e+00\n",
      "39863    0.000000e+00\n",
      "39864    0.000000e+00\n",
      "39865    0.000000e+00\n",
      "39866    0.000000e+00\n",
      "39867    0.000000e+00\n",
      "39868    0.000000e+00\n",
      "39869    0.000000e+00\n",
      "39870    7.238090e-05\n",
      "39871    0.000000e+00\n",
      "39872    8.971651e-20\n",
      "Length: 39873, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "series = pd.Series(predict_list)\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7812  7813  7814 ...  1337 39873  7811]\n"
     ]
    }
   ],
   "source": [
    "#랭킹\n",
    "rankSeries = series.rank(method='first',ascending=False)\n",
    "rank = rankSeries.astype('int32').values\n",
    "print(rank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network를 사용하였을 때 가장 높은 값이 나왔지만, Random Forest같은 경우도 f1:0.842857143, fbeta:0.81595744680851라는 상당한 높은 수치가 나왔으므로 하이퍼 파라미터를 적절히 튜닝한다면 NN급의 성능을 보일 수 있을 것 같다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Project",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

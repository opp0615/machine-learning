{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 앞에서 기계학습의 대표적인 알고리즘의 하나인 회귀(regression)에 대해 공부하였다. 다음의 한가지 예를 우리가 배운 기계학습 알고리즘 풀어보자. \n",
    "\n",
    "먼저 여러분이 입학관리시스템을 reverse engineering 한다고 가정하다. 여러분에게는 세종대학교에 지원한 고등학생들이 학생부 데이터를 갖고 있으며, 학생들의 합격/ 불합격에 대한 데이터도 갖고 있다. 우리가 세종대학교의 입학관리시스템의 알고리즘을 알고 싶다고 할 때,  이 문제를 regression으로 할 수 있을까? \n",
    "\n",
    "안타깝게도 본 문제는 회귀로 학습이 되지 못한다. 우리가 알고 있는 regression에서 dependent variable 은 연속적인 숫자였는데, 이 데이터의 dependent variable 은 binary(True of False)이다! 그렇다면, 우리는 이제 새로운 알고리즘을 공부하여야 한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 classification에서 가장 간단하고 고전적인 로지스틱 회귀에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 Classification은 불연속적인 dependent variable을 결정할 때 사용된다고 설명하였다. 즉dependent variable은 categorical한 값이다. (예시 1, 코끼리,고양이,말, 토끼, 강아지   예시 2 양성 종양, 음성 종양). 당분간 예시1의 multiclass classification 은 미루어두고, 먼저 예시 2의 binary classification에 집중하도록 하자. Binary classification은 dependent variable 값이 2개인 경우를 binary classification이라고 한다. (영어의 접두사 Bi...는 2를 뜻한다. Bicycle은 바퀴가 cycle이 2개이어서 Bicycle이다 ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 그렇다면, 우리가 알고있는 알고리즘을 활용하여 dependent variable을 두개로 나눌 수 있을까? \n",
    "(가능하니까, 물어봤을 것이다...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "               #(the sigmoid function)\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 함수는 출력값을 0과 1 사이에 mapping 시켜주는 함수이다. 이 함수만 잘 사용하면 된다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10., 10., 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x의 입력값들을 먼저 정의하고, 이 값들에 대응하는 y값을 계산하여 그래프로 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid의 형태를 앞의 절에서 배웠다. 이제 이것을 바탕으로 hypothesis(가정)와 cost function을 정의하여, 데이터를 기반으로 학습을 하는 과정을 배우도록하자. 먼저 hypothesis와 cost function을 정의하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 앞에서 regression을 다음과 같이 표현한다고 배웠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $h_θ(x)={θ}^T{x}=θ_0+θ_1x_1+θ_2x_2+⋯+θ_Kx_K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary function에는 sigmoid 함수만 masking하면 된다. (간단하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $h_θ(x)=\\sigma ({θ}^T{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 sigmoid함수는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(z) = {1\\over 1+\\exp(-z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis 함수는 아래와 같이 정의할 수 있으며, 함수가 의미하는 것은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{  h_\\sigma(x) =\n",
    "  \\begin{cases}\n",
    "    >0.5      & \\quad \\text{if }\\theta^Tx>0\\\\\n",
    "    <0.5  & \\quad \\text{if } \\theta^Tx<0\n",
    "  \\end{cases}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식을 보면, 입력값의 가중치 합이 0보다 크면, 예측된 class 는 1이고, 0보다 작으면 그 반대이다. 즉 decision boundary (클래스를 나누는 경계)는 가중치의 합이 0일때가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형회귀 때와 마찬가지로, 이번에도 cost function을 정의하여, 궁극적으로 cost function을 최소화 하며 가중치를 학습할 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 예로 하나의 데이터 포인트로 모델을 학습한다고 할 때 다음과 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{  \\text{cost} =\n",
    "  \\begin{cases}\n",
    "    -\\log(h_\\theta(x))      & \\quad \\text{if }y=1\\\\\n",
    "     -\\log(1-h_\\theta(x))   & \\quad \\text{if }y=0\n",
    "  \\end{cases}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost function에 대한 해석은 다음과 같다. \n",
    "\n",
    "먼저 binary class의 label은 숫자인 0과 1로 표기한다고 할 때, 실제 값이 1이고 모델이 0으로 예측을 하였거나, 실제 값이 0인데 모델이 1로 예측을 하였다면, 우리는 cost 함수를 penalize 할 필요가 있다. 위 함수를 보면 알 수 있듯이 -log(h(x)) 함수의 경우 h(x)가 1로 가까워 질 때, 0으로 수렴하나, h(x)가 0가 0에 가까워지면 무한대에 가까워진다. 반대로 -log(1-h(x))의 함수는 실제 값이 0이고 예측된값이 0이면 0에 수렴하고, 예측값이 1일 때 무한대로 가게 되는 것을 볼 수 있다. 즉 모델이 실제 값을 맞게 예측할 때 cost의 값은 0에 수렴한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 함수를 수학식으로 1라인으로 표기한다고 한다면 어떻게 표현할 수 있을까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다른 접근 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 logistic regression 의 hypothesis 를 보도록 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_θ(x)=\\sigma({θ}^T{x})=\\sigma(θ_0+θ_1x_1+θ_2x_2+⋯+θ_Kx_K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면, logistic regression 모델이 주어졌을 때, 어떻게 하여 모델의 파라미터들의 fitting이 가능할까? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 문제를 풀기 위하여, 확률적인 가정과 수식을 이용하여 $\\theta$를 구해보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 다음과 같이 가정하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(y = 1 | x; θ) = h_{θ}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이라고 할 때, 반대의 개념은 다음과 같이 쉽게 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(y = 0 | x; θ) = 1-h_{θ}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면, 두개의 식을 하나로 합치면 다음과 같을 것이다 (지난 주와 유사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(y | x; θ) = (h_θ(x))^y(1 − h_θ(x))^{(1−y)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면, m개의 데이터 샘플이 있다고 하면, likelihood 함수를 어떻게 작성할 수 있을까? likelihood가 무엇인지 모르는 학생들은 우도, 가능도를 찾아보세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "L(\\theta)  &= p(Y | X; θ) \\\\\n",
    "           &= \\prod_i^m(y^{(i)} | x^{(i)};\\theta) \\\\\n",
    "           &= \\prod_i^m (h_θ(x^{(i)}))^{y^{(i)}}(1 − h_θ(x^{(i)}))^{(1−y^{(i)})} \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계 학습에서 likelihood 함수를 최대화 하는데, 보통 계산적인 편의성을 위하여, log 를 취하여 최대화 한다. log 함수는 증가이므로, log에서 최대 값이 되는 input 을 구하면, 그 input에 대응되는 값이 최대값이 된다! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 위의 likelihood 함수에 log를 취하여 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "l(\\theta)  &= \\log L(\\theta) \\\\\n",
    "            &=\\sum_i^m  y^{(i)} \\log(h_θ(x^{(i)})) +  (1-y^{(i)})\\log(1 − h_θ(x^{(i)}))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난주에 배웠던 위의 cost 와 비교해보자. 어떤 관계가 있는 것을 알 수 있나?\n",
    "그렇다 likelihood를 maximize 하는게 곧 cost 함수를 minimize 하는 것임을 알 수 있다. (사실 likelihood에서 먼저 출발하여 이해하는 것이 더 빠르며 직관적이다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면, 이제부터 어떻게 접근하면, 적절한 $\\theta$ 를 구할 수 있을까? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD 를 사용함!  (강의 시간에 유도)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 그렇다면, scikie learn에서는 어떤 식으로 제공되는지 먼저 scikit learn에서 logistic regression을 실행해보자. (과제는 당연히 직접 구현하여 풀어보는 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 scikie learn에서 logistic regression 을 import 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pima indian diabetes 데이터를 사용하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label\n",
       "0         6      148  72    35        0  33.6     0.627   50      1\n",
       "1         1       85  66    29        0  26.6     0.351   31      0\n",
       "2         8      183  64     0        0  23.3     0.672   32      1\n",
       "3         1       89  66    23       94  28.1     0.167   21      0\n",
       "4         0      137  40    35      168  43.1     2.288   33      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"diabetes.csv\", header=0, names=col_names)\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>age</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>pedigree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>50</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>31</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>0.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>32</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>21</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>33</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>2.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>30</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>29</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>53</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>30</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>34</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>57</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>1.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>59</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>51</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>0.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>31</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>0.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>31</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>33</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>32</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>27</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>50</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>41</td>\n",
       "      <td>196</td>\n",
       "      <td>90</td>\n",
       "      <td>0.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29</td>\n",
       "      <td>119</td>\n",
       "      <td>80</td>\n",
       "      <td>0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>36.6</td>\n",
       "      <td>51</td>\n",
       "      <td>143</td>\n",
       "      <td>94</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>31.1</td>\n",
       "      <td>41</td>\n",
       "      <td>125</td>\n",
       "      <td>70</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>43</td>\n",
       "      <td>147</td>\n",
       "      <td>76</td>\n",
       "      <td>0.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>23.2</td>\n",
       "      <td>22</td>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>110</td>\n",
       "      <td>22.2</td>\n",
       "      <td>57</td>\n",
       "      <td>145</td>\n",
       "      <td>82</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>38</td>\n",
       "      <td>117</td>\n",
       "      <td>92</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>36.6</td>\n",
       "      <td>21</td>\n",
       "      <td>99</td>\n",
       "      <td>60</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>42</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>42.3</td>\n",
       "      <td>48</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>30.8</td>\n",
       "      <td>26</td>\n",
       "      <td>102</td>\n",
       "      <td>44</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>28.5</td>\n",
       "      <td>22</td>\n",
       "      <td>109</td>\n",
       "      <td>58</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>45</td>\n",
       "      <td>140</td>\n",
       "      <td>94</td>\n",
       "      <td>0.734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>40.6</td>\n",
       "      <td>39</td>\n",
       "      <td>153</td>\n",
       "      <td>88</td>\n",
       "      <td>1.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>30.0</td>\n",
       "      <td>46</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>94</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>46.3</td>\n",
       "      <td>32</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>1.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>36.4</td>\n",
       "      <td>36</td>\n",
       "      <td>187</td>\n",
       "      <td>70</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>50</td>\n",
       "      <td>162</td>\n",
       "      <td>62</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>22</td>\n",
       "      <td>136</td>\n",
       "      <td>70</td>\n",
       "      <td>1.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28</td>\n",
       "      <td>121</td>\n",
       "      <td>78</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25</td>\n",
       "      <td>108</td>\n",
       "      <td>62</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>43.3</td>\n",
       "      <td>26</td>\n",
       "      <td>181</td>\n",
       "      <td>88</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>45</td>\n",
       "      <td>154</td>\n",
       "      <td>78</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>1.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39</td>\n",
       "      <td>137</td>\n",
       "      <td>90</td>\n",
       "      <td>0.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>52</td>\n",
       "      <td>123</td>\n",
       "      <td>72</td>\n",
       "      <td>0.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>26</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>66</td>\n",
       "      <td>190</td>\n",
       "      <td>92</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>28.4</td>\n",
       "      <td>22</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>43</td>\n",
       "      <td>170</td>\n",
       "      <td>74</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>62</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>63</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>27</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>30</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>47</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>23</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>0.315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pregnant  insulin   bmi  age  glucose  bp  pedigree\n",
       "0           6        0  33.6   50      148  72     0.627\n",
       "1           1        0  26.6   31       85  66     0.351\n",
       "2           8        0  23.3   32      183  64     0.672\n",
       "3           1       94  28.1   21       89  66     0.167\n",
       "4           0      168  43.1   33      137  40     2.288\n",
       "5           5        0  25.6   30      116  74     0.201\n",
       "6           3       88  31.0   26       78  50     0.248\n",
       "7          10        0  35.3   29      115   0     0.134\n",
       "8           2      543  30.5   53      197  70     0.158\n",
       "9           8        0   0.0   54      125  96     0.232\n",
       "10          4        0  37.6   30      110  92     0.191\n",
       "11         10        0  38.0   34      168  74     0.537\n",
       "12         10        0  27.1   57      139  80     1.441\n",
       "13          1      846  30.1   59      189  60     0.398\n",
       "14          5      175  25.8   51      166  72     0.587\n",
       "15          7        0  30.0   32      100   0     0.484\n",
       "16          0      230  45.8   31      118  84     0.551\n",
       "17          7        0  29.6   31      107  74     0.254\n",
       "18          1       83  43.3   33      103  30     0.183\n",
       "19          1       96  34.6   32      115  70     0.529\n",
       "20          3      235  39.3   27      126  88     0.704\n",
       "21          8        0  35.4   50       99  84     0.388\n",
       "22          7        0  39.8   41      196  90     0.451\n",
       "23          9        0  29.0   29      119  80     0.263\n",
       "24         11      146  36.6   51      143  94     0.254\n",
       "25         10      115  31.1   41      125  70     0.205\n",
       "26          7        0  39.4   43      147  76     0.257\n",
       "27          1      140  23.2   22       97  66     0.487\n",
       "28         13      110  22.2   57      145  82     0.245\n",
       "29          5        0  34.1   38      117  92     0.337\n",
       "..        ...      ...   ...  ...      ...  ..       ...\n",
       "738         2      160  36.6   21       99  60     0.453\n",
       "739         1        0  39.5   42      102  74     0.293\n",
       "740        11      150  42.3   48      120  80     0.785\n",
       "741         3       94  30.8   26      102  44     0.400\n",
       "742         1      116  28.5   22      109  58     0.219\n",
       "743         9        0  32.7   45      140  94     0.734\n",
       "744        13      140  40.6   39      153  88     1.174\n",
       "745        12      105  30.0   46      100  84     0.488\n",
       "746         1        0  49.3   27      147  94     0.358\n",
       "747         1       57  46.3   32       81  74     1.096\n",
       "748         3      200  36.4   36      187  70     0.408\n",
       "749         6        0  24.3   50      162  62     0.178\n",
       "750         4        0  31.2   22      136  70     1.182\n",
       "751         1       74  39.0   28      121  78     0.261\n",
       "752         3        0  26.0   25      108  62     0.223\n",
       "753         0      510  43.3   26      181  88     0.222\n",
       "754         8        0  32.4   45      154  78     0.443\n",
       "755         1      110  36.5   37      128  88     1.057\n",
       "756         7        0  32.0   39      137  90     0.391\n",
       "757         0        0  36.3   52      123  72     0.258\n",
       "758         1        0  37.5   26      106  76     0.197\n",
       "759         6        0  35.5   66      190  92     0.278\n",
       "760         2       16  28.4   22       88  58     0.766\n",
       "761         9        0  44.0   43      170  74     0.403\n",
       "762         9        0  22.5   33       89  62     0.142\n",
       "763        10      180  32.9   63      101  76     0.171\n",
       "764         2        0  36.8   27      122  70     0.340\n",
       "765         5      112  26.2   30      121  72     0.245\n",
       "766         1        0  30.1   47      126  60     0.349\n",
       "767         1        0  30.4   23       93  70     0.315\n",
       "\n",
       "[768 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "5      0\n",
       "6      1\n",
       "7      0\n",
       "8      1\n",
       "9      1\n",
       "10     0\n",
       "11     1\n",
       "12     0\n",
       "13     1\n",
       "14     1\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18     0\n",
       "19     1\n",
       "20     0\n",
       "21     0\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "738    0\n",
       "739    1\n",
       "740    1\n",
       "741    0\n",
       "742    0\n",
       "743    1\n",
       "744    0\n",
       "745    0\n",
       "746    1\n",
       "747    0\n",
       "748    1\n",
       "749    1\n",
       "750    1\n",
       "751    0\n",
       "752    0\n",
       "753    1\n",
       "754    1\n",
       "755    1\n",
       "756    0\n",
       "757    1\n",
       "758    0\n",
       "759    1\n",
       "760    0\n",
       "761    1\n",
       "762    0\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    1\n",
       "767    0\n",
       "Name: label, Length: 768, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traing set 과 test set을 나누자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cloto\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred=logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11559877, 0.88440123],\n",
       "       [0.79010942, 0.20989058],\n",
       "       [0.84363562, 0.15636438],\n",
       "       [0.40194611, 0.59805389],\n",
       "       [0.81809255, 0.18190745],\n",
       "       [0.9232763 , 0.0767237 ],\n",
       "       [0.31770178, 0.68229822],\n",
       "       [0.25723209, 0.74276791],\n",
       "       [0.54893027, 0.45106973],\n",
       "       [0.6298874 , 0.3701126 ],\n",
       "       [0.45269669, 0.54730331],\n",
       "       [0.10096342, 0.89903658],\n",
       "       [0.68734339, 0.31265661],\n",
       "       [0.74785408, 0.25214592],\n",
       "       [0.82909655, 0.17090345],\n",
       "       [0.79896614, 0.20103386],\n",
       "       [0.21375773, 0.78624227],\n",
       "       [0.93066837, 0.06933163],\n",
       "       [0.63680749, 0.36319251],\n",
       "       [0.68877062, 0.31122938],\n",
       "       [0.44885404, 0.55114596],\n",
       "       [0.60431005, 0.39568995],\n",
       "       [0.65972928, 0.34027072],\n",
       "       [0.90845349, 0.09154651],\n",
       "       [0.89179988, 0.10820012],\n",
       "       [0.63754885, 0.36245115],\n",
       "       [0.90935342, 0.09064658],\n",
       "       [0.16951757, 0.83048243],\n",
       "       [0.8317368 , 0.1682632 ],\n",
       "       [0.78762221, 0.21237779],\n",
       "       [0.54027089, 0.45972911],\n",
       "       [0.7342403 , 0.2657597 ],\n",
       "       [0.86247122, 0.13752878],\n",
       "       [0.53868117, 0.46131883],\n",
       "       [0.82478501, 0.17521499],\n",
       "       [0.34573441, 0.65426559],\n",
       "       [0.54633147, 0.45366853],\n",
       "       [0.86826034, 0.13173966],\n",
       "       [0.63886716, 0.36113284],\n",
       "       [0.32437643, 0.67562357],\n",
       "       [0.68132503, 0.31867497],\n",
       "       [0.75133469, 0.24866531],\n",
       "       [0.76653703, 0.23346297],\n",
       "       [0.26785331, 0.73214669],\n",
       "       [0.2895407 , 0.7104593 ],\n",
       "       [0.97018889, 0.02981111],\n",
       "       [0.85585741, 0.14414259],\n",
       "       [0.73312257, 0.26687743],\n",
       "       [0.63107359, 0.36892641],\n",
       "       [0.67351552, 0.32648448],\n",
       "       [0.55336092, 0.44663908],\n",
       "       [0.74518771, 0.25481229],\n",
       "       [0.18262095, 0.81737905],\n",
       "       [0.5742647 , 0.4257353 ],\n",
       "       [0.81066739, 0.18933261],\n",
       "       [0.98735081, 0.01264919],\n",
       "       [0.89685408, 0.10314592],\n",
       "       [0.54194808, 0.45805192],\n",
       "       [0.6786751 , 0.3213249 ],\n",
       "       [0.77236192, 0.22763808],\n",
       "       [0.38638034, 0.61361966],\n",
       "       [0.5138712 , 0.4861288 ],\n",
       "       [0.82985816, 0.17014184],\n",
       "       [0.27290942, 0.72709058],\n",
       "       [0.37303584, 0.62696416],\n",
       "       [0.15654727, 0.84345273],\n",
       "       [0.36144874, 0.63855126],\n",
       "       [0.80548638, 0.19451362],\n",
       "       [0.58202332, 0.41797668],\n",
       "       [0.84271567, 0.15728433],\n",
       "       [0.81084099, 0.18915901],\n",
       "       [0.46601838, 0.53398162],\n",
       "       [0.86480412, 0.13519588],\n",
       "       [0.12481579, 0.87518421],\n",
       "       [0.23711319, 0.76288681],\n",
       "       [0.66613221, 0.33386779],\n",
       "       [0.86022793, 0.13977207],\n",
       "       [0.40891203, 0.59108797],\n",
       "       [0.89143183, 0.10856817],\n",
       "       [0.79397949, 0.20602051],\n",
       "       [0.65150455, 0.34849545],\n",
       "       [0.61704245, 0.38295755],\n",
       "       [0.75025395, 0.24974605],\n",
       "       [0.92282066, 0.07717934],\n",
       "       [0.75534909, 0.24465091],\n",
       "       [0.75776972, 0.24223028],\n",
       "       [0.63986638, 0.36013362],\n",
       "       [0.56809505, 0.43190495],\n",
       "       [0.20812271, 0.79187729],\n",
       "       [0.77628077, 0.22371923],\n",
       "       [0.80287518, 0.19712482],\n",
       "       [0.77909228, 0.22090772],\n",
       "       [0.6873504 , 0.3126496 ],\n",
       "       [0.91554085, 0.08445915],\n",
       "       [0.46952863, 0.53047137],\n",
       "       [0.75185391, 0.24814609],\n",
       "       [0.59191787, 0.40808213],\n",
       "       [0.5272734 , 0.4727266 ],\n",
       "       [0.46760989, 0.53239011],\n",
       "       [0.7176745 , 0.2823255 ],\n",
       "       [0.71966923, 0.28033077],\n",
       "       [0.82926901, 0.17073099],\n",
       "       [0.76157619, 0.23842381],\n",
       "       [0.91719313, 0.08280687],\n",
       "       [0.45104026, 0.54895974],\n",
       "       [0.60725851, 0.39274149],\n",
       "       [0.82122124, 0.17877876],\n",
       "       [0.69669046, 0.30330954],\n",
       "       [0.91329474, 0.08670526],\n",
       "       [0.28080692, 0.71919308],\n",
       "       [0.83497141, 0.16502859],\n",
       "       [0.65964859, 0.34035141],\n",
       "       [0.41498592, 0.58501408],\n",
       "       [0.66057399, 0.33942601],\n",
       "       [0.45230151, 0.54769849],\n",
       "       [0.44411424, 0.55588576],\n",
       "       [0.79475119, 0.20524881],\n",
       "       [0.31377986, 0.68622014],\n",
       "       [0.86276612, 0.13723388],\n",
       "       [0.33758658, 0.66241342],\n",
       "       [0.66120959, 0.33879041],\n",
       "       [0.72110714, 0.27889286],\n",
       "       [0.68196444, 0.31803556],\n",
       "       [0.55825722, 0.44174278],\n",
       "       [0.73864764, 0.26135236],\n",
       "       [0.90061051, 0.09938949],\n",
       "       [0.65925605, 0.34074395],\n",
       "       [0.64693016, 0.35306984],\n",
       "       [0.53911398, 0.46088602],\n",
       "       [0.62087328, 0.37912672],\n",
       "       [0.5921043 , 0.4078957 ],\n",
       "       [0.85399284, 0.14600716],\n",
       "       [0.89333753, 0.10666247],\n",
       "       [0.27803871, 0.72196129],\n",
       "       [0.66332774, 0.33667226],\n",
       "       [0.62022756, 0.37977244],\n",
       "       [0.79995212, 0.20004788],\n",
       "       [0.6182457 , 0.3817543 ],\n",
       "       [0.29758344, 0.70241656],\n",
       "       [0.72680954, 0.27319046],\n",
       "       [0.85086373, 0.14913627],\n",
       "       [0.49188035, 0.50811965],\n",
       "       [0.86767217, 0.13232783],\n",
       "       [0.87420341, 0.12579659],\n",
       "       [0.62278991, 0.37721009],\n",
       "       [0.85109197, 0.14890803],\n",
       "       [0.86988654, 0.13011346],\n",
       "       [0.85831137, 0.14168863],\n",
       "       [0.8175771 , 0.1824229 ],\n",
       "       [0.77384544, 0.22615456],\n",
       "       [0.856565  , 0.143435  ],\n",
       "       [0.42563143, 0.57436857],\n",
       "       [0.8336974 , 0.1663026 ],\n",
       "       [0.78051129, 0.21948871],\n",
       "       [0.32617883, 0.67382117],\n",
       "       [0.81222908, 0.18777092],\n",
       "       [0.33788167, 0.66211833],\n",
       "       [0.78722305, 0.21277695],\n",
       "       [0.37292647, 0.62707353],\n",
       "       [0.06231351, 0.93768649],\n",
       "       [0.3673005 , 0.6326995 ],\n",
       "       [0.30851212, 0.69148788],\n",
       "       [0.93864367, 0.06135633],\n",
       "       [0.68319635, 0.31680365],\n",
       "       [0.20345976, 0.79654024],\n",
       "       [0.55565745, 0.44434255],\n",
       "       [0.71409411, 0.28590589],\n",
       "       [0.80967838, 0.19032162],\n",
       "       [0.68426835, 0.31573165],\n",
       "       [0.86703228, 0.13296772],\n",
       "       [0.78930661, 0.21069339],\n",
       "       [0.75220994, 0.24779006],\n",
       "       [0.75304531, 0.24695469],\n",
       "       [0.73584628, 0.26415372],\n",
       "       [0.54470371, 0.45529629],\n",
       "       [0.81870419, 0.18129581],\n",
       "       [0.61041543, 0.38958457],\n",
       "       [0.86169384, 0.13830616],\n",
       "       [0.82440438, 0.17559562],\n",
       "       [0.88485828, 0.11514172],\n",
       "       [0.79752632, 0.20247368],\n",
       "       [0.28319112, 0.71680888],\n",
       "       [0.80896495, 0.19103505],\n",
       "       [0.1279735 , 0.8720265 ],\n",
       "       [0.50297553, 0.49702447],\n",
       "       [0.93611543, 0.06388457],\n",
       "       [0.30843107, 0.69156893],\n",
       "       [0.69183584, 0.30816416],\n",
       "       [0.55835232, 0.44164768],\n",
       "       [0.78970027, 0.21029973],\n",
       "       [0.76922306, 0.23077694],\n",
       "       [0.86656309, 0.13343691]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661    1\n",
       "122    0\n",
       "113    0\n",
       "14     1\n",
       "529    0\n",
       "103    0\n",
       "338    1\n",
       "588    1\n",
       "395    0\n",
       "204    0\n",
       "31     1\n",
       "546    1\n",
       "278    0\n",
       "593    0\n",
       "737    0\n",
       "202    0\n",
       "175    1\n",
       "55     0\n",
       "479    0\n",
       "365    0\n",
       "417    1\n",
       "577    1\n",
       "172    0\n",
       "352    0\n",
       "27     0\n",
       "605    0\n",
       "239    0\n",
       "744    0\n",
       "79     0\n",
       "496    0\n",
       "      ..\n",
       "97     0\n",
       "530    0\n",
       "327    0\n",
       "619    1\n",
       "518    0\n",
       "632    0\n",
       "524    0\n",
       "536    0\n",
       "597    0\n",
       "462    0\n",
       "17     1\n",
       "739    1\n",
       "263    0\n",
       "241    0\n",
       "344    0\n",
       "302    0\n",
       "704    0\n",
       "240    0\n",
       "170    1\n",
       "691    1\n",
       "490    0\n",
       "45     1\n",
       "750    1\n",
       "62     0\n",
       "78     1\n",
       "366    1\n",
       "301    1\n",
       "382    0\n",
       "140    0\n",
       "463    0\n",
       "Name: label, Length: 192, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = y_test - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.absolute(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 21,  27,  36,  39,  47,  48,  49,  57,  58,  59,  71,  73,  77,\n",
       "         86,  94,  96,  99, 104, 105, 111, 113, 117, 127, 135, 137, 141,\n",
       "        144, 149, 156, 164, 165, 172, 173, 180, 184, 187, 188], dtype=int64),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(temp == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.sum() #틀린 개수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.270833333333336"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "37/192*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix 란? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 인데 1로 예측, 0인데 0으로 예측, 1인데 1로 예측, 1인데 0으로 예측한 것에 가중치를 준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import matrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
